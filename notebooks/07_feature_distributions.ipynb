{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 7: Feature Distributions\n",
        "\n",
        "## Purpose\n",
        "Understand the distribution and characteristics of each feature in the PathWild dataset.\n",
        "\n",
        "## Key Questions\n",
        "- Which features are normally distributed vs heavily skewed?\n",
        "- Do any features need transformation?\n",
        "- Which features show strongest separation between presence/absence?\n",
        "- Are there unexpected bimodal distributions?\n",
        "\n",
        "## Key Observations to Look For\n",
        "- **Normal vs Skewed**: Identify features needing transformation\n",
        "- **Seasonal Patterns**: NDVI should peak in summer, snow in winter\n",
        "- **Presence/Absence Separation**: Features with clear separation are good predictors\n",
        "- **Statistical Significance**: p-values and effect sizes for discrimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import shapiro, ttest_ind\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Determine project root and output directories\n",
        "possible_roots = [\n",
        "    Path('.'),  # If running from project root\n",
        "    Path('..'),  # If running from notebooks directory\n",
        "    Path('../..'),  # If running from subdirectory\n",
        "]\n",
        "\n",
        "data_root = None\n",
        "for root in possible_roots:\n",
        "    if (root / 'data' / 'features').exists():\n",
        "        data_root = root / 'data'\n",
        "        break\n",
        "\n",
        "if data_root is None:\n",
        "    data_root = Path('../data')\n",
        "\n",
        "# Create output directories relative to project root\n",
        "figures_dir = data_root / 'figures'\n",
        "reports_dir = data_root / 'reports'\n",
        "figures_dir.mkdir(parents=True, exist_ok=True)\n",
        "reports_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f'✓ Setup complete')\n",
        "print(f'  Output directory: {data_root.absolute()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data and Detect Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "from pathlib import Path\n",
        "\n",
        "# Try multiple possible paths\n",
        "possible_paths = [\n",
        "    Path('data/features/complete_context.csv'),  # From project root\n",
        "    Path('../data/features/complete_context.csv'),  # From notebooks directory\n",
        "    Path('../../data/features/complete_context.csv'),  # From subdirectory\n",
        "]\n",
        "\n",
        "data_path = None\n",
        "for path in possible_paths:\n",
        "    if path.exists():\n",
        "        data_path = path\n",
        "        break\n",
        "\n",
        "if data_path is None:\n",
        "    raise FileNotFoundError(\n",
        "        f'Data file not found. Tried: {[str(p) for p in possible_paths]}\\n'\n",
        "        f'Please run: python scripts/combine_feature_files.py\\n'\n",
        "        f'Or ensure you are running the notebook from the project root directory.'\n",
        "    )\n",
        "\n",
        "print(f'Loading data from: {data_path}')\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Detect key columns\n",
        "timestamp_col = None\n",
        "presence_col = None\n",
        "month_col = None\n",
        "\n",
        "# Look for timestamp\n",
        "for col in df.columns:\n",
        "    if any(x in col.lower() for x in ['timestamp', 'date', 'time']):\n",
        "        timestamp_col = col\n",
        "        break\n",
        "\n",
        "# Look for month column (may exist directly in data)\n",
        "if 'month' in df.columns:\n",
        "    month_col = 'month'\n",
        "elif timestamp_col:\n",
        "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce')\n",
        "    df['month'] = df[timestamp_col].dt.month\n",
        "    month_col = 'month'\n",
        "\n",
        "# Look for presence/target\n",
        "for col in df.columns:\n",
        "    if col.lower() in ['presence', 'target', 'label', 'is_presence', 'elk_present']:\n",
        "        presence_col = col\n",
        "        break\n",
        "\n",
        "print(f'Dataset shape: {df.shape}')\n",
        "print(f'Timestamp column: {timestamp_col}')\n",
        "print(f'Month column: {month_col}')\n",
        "print(f'Presence column: {presence_col}')\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "# Exclude index columns\n",
        "exclude_cols = ['point_index']\n",
        "if 'year' in df.columns and df['year'].dtype in [np.int64, np.float64]:\n",
        "    exclude_cols.append('year')\n",
        "\n",
        "numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns.tolist() \n",
        "                if col not in exclude_cols]\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "print(f'\\nNumeric columns: {len(numeric_cols)}')\n",
        "print(f'Categorical columns: {len(categorical_cols)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Univariate Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create histograms for all numeric features\n",
        "n_cols = 3\n",
        "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "\n",
        "for idx, col in enumerate(numeric_cols):\n",
        "    ax = axes[idx]\n",
        "    data = df[col].dropna()\n",
        "    \n",
        "    if len(data) > 0:\n",
        "        # Histogram with KDE\n",
        "        ax.hist(data, bins=50, alpha=0.7, color='steelblue', edgecolor='black', density=True)\n",
        "        \n",
        "        # KDE overlay\n",
        "        try:\n",
        "            from scipy.stats import gaussian_kde\n",
        "            kde = gaussian_kde(data)\n",
        "            x_range = np.linspace(data.min(), data.max(), 100)\n",
        "            ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Add statistics\n",
        "        mean_val = data.mean()\n",
        "        median_val = data.median()\n",
        "        std_val = data.std()\n",
        "        \n",
        "        ax.axvline(mean_val, color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
        "        ax.axvline(median_val, color='blue', linestyle='--', linewidth=1, alpha=0.7)\n",
        "        \n",
        "        ax.set_title(f'{col}\\nMean: {mean_val:.2f}, Std: {std_val:.2f}', fontsize=10)\n",
        "        ax.set_xlabel(col, fontsize=9)\n",
        "        ax.set_ylabel('Density', fontsize=9)\n",
        "        ax.grid(alpha=0.3)\n",
        "\n",
        "# Hide extra subplots\n",
        "for idx in range(len(numeric_cols), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Feature Distributions (Histograms with KDE)', fontsize=16, y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.savefig(figures_dir / 'feature_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Saved feature distributions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify skewed features\n",
        "skewness_data = []\n",
        "\n",
        "for col in numeric_cols:\n",
        "    data = df[col].dropna()\n",
        "    if len(data) > 0:\n",
        "        skew = data.skew()\n",
        "        skewness_data.append({'feature': col, 'skewness': skew})\n",
        "\n",
        "skewness_df = pd.DataFrame(skewness_data).sort_values('skewness', key=abs, ascending=False)\n",
        "\n",
        "print('\\nFeature skewness (sorted by absolute value):')\n",
        "print(skewness_df)\n",
        "\n",
        "highly_skewed = skewness_df[abs(skewness_df['skewness']) > 1]\n",
        "print(f'\\n⚠ {len(highly_skewed)} features with |skewness| > 1 (may need transformation):')\n",
        "for _, row in highly_skewed.iterrows():\n",
        "    print(f\"  - {row['feature']}: {row['skewness']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Categorical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze categorical features\n",
        "if len(categorical_cols) > 0:\n",
        "    for col in categorical_cols:\n",
        "        if col not in [timestamp_col]:  # Skip timestamp\n",
        "            print(f'\\n{col}:')\n",
        "            print(df[col].value_counts())\n",
        "            \n",
        "            # Create count plot\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            value_counts = df[col].value_counts()\n",
        "            plt.bar(range(len(value_counts)), value_counts.values, color='steelblue', alpha=0.7)\n",
        "            plt.xlabel(col, fontsize=12)\n",
        "            plt.ylabel('Count', fontsize=12)\n",
        "            plt.title(f'{col} Distribution', fontsize=14)\n",
        "            plt.xticks(range(len(value_counts)), value_counts.index, rotation=45, ha='right')\n",
        "            plt.grid(axis='y', alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(figures_dir / f'{col}_distribution.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "else:\n",
        "    print('No categorical features found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Seasonal Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze seasonal patterns\n",
        "if timestamp_col and 'month' in df.columns:\n",
        "    # Select key features for seasonal analysis\n",
        "    seasonal_features = [col for col in numeric_cols if col not in ['month', 'year']][:12]\n",
        "    \n",
        "    n_cols = 3\n",
        "    n_rows = (len(seasonal_features) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "    \n",
        "    for idx, col in enumerate(seasonal_features):\n",
        "        ax = axes[idx]\n",
        "        monthly_mean = df.groupby('month')[col].mean()\n",
        "        \n",
        "        ax.plot(monthly_mean.index, monthly_mean.values, marker='o', linewidth=2, markersize=8)\n",
        "        ax.set_xlabel('Month', fontsize=10)\n",
        "        ax.set_ylabel(f'Mean {col}', fontsize=10)\n",
        "        ax.set_title(col, fontsize=11)\n",
        "        ax.set_xticks(range(1, 13))\n",
        "        ax.set_xticklabels(['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D'])\n",
        "        ax.grid(alpha=0.3)\n",
        "    \n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(seasonal_features), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle('Seasonal Patterns (Monthly Means)', fontsize=16, y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'seasonal_patterns.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('✓ Saved seasonal patterns')\n",
        "else:\n",
        "    print('Cannot analyze seasonal patterns without timestamp column')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Presence vs Absence Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare distributions for presence vs absence\n",
        "if presence_col:\n",
        "    # Select top features by variance for visualization\n",
        "    feature_variance = df[numeric_cols].var().sort_values(ascending=False)\n",
        "    top_features = feature_variance.head(8).index.tolist()\n",
        "    \n",
        "    fig, axes = plt.subplots(4, 2, figsize=(14, 16))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, col in enumerate(top_features):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        presence_data = df[df[presence_col] == 1][col].dropna()\n",
        "        absence_data = df[df[presence_col] == 0][col].dropna()\n",
        "        \n",
        "        # Overlapping histograms\n",
        "        ax.hist(presence_data, bins=30, alpha=0.5, color='blue', label='Presence', density=True)\n",
        "        ax.hist(absence_data, bins=30, alpha=0.5, color='red', label='Absence', density=True)\n",
        "        \n",
        "        # Add means\n",
        "        ax.axvline(presence_data.mean(), color='blue', linestyle='--', linewidth=2)\n",
        "        ax.axvline(absence_data.mean(), color='red', linestyle='--', linewidth=2)\n",
        "        \n",
        "        # Calculate mean difference\n",
        "        mean_diff = presence_data.mean() - absence_data.mean()\n",
        "        \n",
        "        ax.set_xlabel(col, fontsize=10)\n",
        "        ax.set_ylabel('Density', fontsize=10)\n",
        "        ax.set_title(f'{col}\\nMean Diff: {mean_diff:.2f}', fontsize=11)\n",
        "        ax.legend()\n",
        "        ax.grid(alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('Presence vs Absence Distributions', fontsize=16, y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'presence_absence_distributions.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('✓ Saved presence vs absence distributions')\n",
        "else:\n",
        "    print('Cannot compare presence/absence without target column')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Statistical Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform t-tests for presence vs absence\n",
        "if presence_col:\n",
        "    test_results = []\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        presence_data = df[df[presence_col] == 1][col].dropna()\n",
        "        absence_data = df[df[presence_col] == 0][col].dropna()\n",
        "        \n",
        "        if len(presence_data) > 0 and len(absence_data) > 0:\n",
        "            # T-test\n",
        "            t_stat, p_value = ttest_ind(presence_data, absence_data)\n",
        "            \n",
        "            # Cohen's d (effect size)\n",
        "            mean_diff = presence_data.mean() - absence_data.mean()\n",
        "            pooled_std = np.sqrt((presence_data.std()**2 + absence_data.std()**2) / 2)\n",
        "            cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0\n",
        "            \n",
        "            test_results.append({\n",
        "                'feature': col,\n",
        "                'p_value': p_value,\n",
        "                'cohens_d': cohens_d,\n",
        "                'presence_mean': presence_data.mean(),\n",
        "                'absence_mean': absence_data.mean(),\n",
        "                'mean_diff': mean_diff\n",
        "            })\n",
        "    \n",
        "    results_df = pd.DataFrame(test_results).sort_values('p_value')\n",
        "    \n",
        "    print('\\nStatistical test results (sorted by p-value):')\n",
        "    print(results_df)\n",
        "    \n",
        "    # Save results\n",
        "    results_df.to_csv(reports_dir / 'feature_discrimination.csv', index=False)\n",
        "    print('\\n✓ Saved feature discrimination results')\n",
        "    \n",
        "    # Flag highly discriminative features\n",
        "    significant = results_df[(results_df['p_value'] < 0.01) & (abs(results_df['cohens_d']) > 0.5)]\n",
        "    print(f'\\n✓ {len(significant)} features with p<0.01 and |Cohen\\'s d|>0.5:')\n",
        "    for _, row in significant.iterrows():\n",
        "        print(f\"  - {row['feature']}: p={row['p_value']:.2e}, d={row['cohens_d']:.2f}\")\n",
        "else:\n",
        "    print('Cannot perform statistical tests without target column')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Normality Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Q-Q plots for features with strong skew\n",
        "from scipy.stats import probplot\n",
        "\n",
        "highly_skewed_features = skewness_df[abs(skewness_df['skewness']) > 1]['feature'].head(6).tolist()\n",
        "\n",
        "if len(highly_skewed_features) > 0:\n",
        "    n_cols = 3\n",
        "    n_rows = (len(highly_skewed_features) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "    \n",
        "    for idx, col in enumerate(highly_skewed_features):\n",
        "        ax = axes[idx]\n",
        "        data = df[col].dropna()\n",
        "        \n",
        "        if len(data) > 0:\n",
        "            probplot(data, dist=\"norm\", plot=ax)\n",
        "            ax.set_title(f'{col}\\n(Skew: {data.skew():.2f})', fontsize=11)\n",
        "            ax.grid(alpha=0.3)\n",
        "    \n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(highly_skewed_features), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle('Q-Q Plots for Skewed Features (Normal Distribution)', fontsize=16, y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'normality_qqplots.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('✓ Saved Q-Q plots for skewed features')\n",
        "else:\n",
        "    print('No highly skewed features found for Q-Q plots')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test normality for each feature\n",
        "normality_results = []\n",
        "\n",
        "for col in numeric_cols[:20]:  # Limit to first 20 for performance\n",
        "    data = df[col].dropna()\n",
        "    if len(data) > 3 and len(data) < 5000:  # Shapiro-Wilk works best with smaller samples\n",
        "        stat, p_value = shapiro(data)\n",
        "        normality_results.append({\n",
        "            'feature': col,\n",
        "            'shapiro_stat': stat,\n",
        "            'p_value': p_value,\n",
        "            'is_normal': p_value > 0.05\n",
        "        })\n",
        "\n",
        "if len(normality_results) > 0:\n",
        "    normality_df = pd.DataFrame(normality_results).sort_values('p_value', ascending=False)\n",
        "    \n",
        "    print('\\nNormality test results:')\n",
        "    print(normality_df)\n",
        "    \n",
        "    non_normal = normality_df[normality_df['p_value'] < 0.05]\n",
        "    print(f'\\n⚠ {len(non_normal)} features fail normality test (p<0.05)')\n",
        "    print('These may benefit from transformation (log, sqrt, box-cox)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive feature summary\n",
        "summary_data = []\n",
        "\n",
        "for col in numeric_cols:\n",
        "    data = df[col].dropna()\n",
        "    \n",
        "    if len(data) > 0:\n",
        "        summary = {\n",
        "            'feature': col,\n",
        "            'dtype': df[col].dtype,\n",
        "            'missing_pct': (df[col].isnull().sum() / len(df) * 100),\n",
        "            'min': data.min(),\n",
        "            'max': data.max(),\n",
        "            'mean': data.mean(),\n",
        "            'std': data.std(),\n",
        "            'skewness': data.skew(),\n",
        "            'kurtosis': data.kurtosis()\n",
        "        }\n",
        "        \n",
        "        # Add normality test if available\n",
        "        norm_result = [r for r in normality_results if r['feature'] == col]\n",
        "        if norm_result:\n",
        "            summary['normality_p'] = norm_result[0]['p_value']\n",
        "        \n",
        "        # Add discrimination metrics if available\n",
        "        if presence_col:\n",
        "            disc_result = results_df[results_df['feature'] == col]\n",
        "            if len(disc_result) > 0:\n",
        "                summary['discrimination_p'] = disc_result.iloc[0]['p_value']\n",
        "                summary['cohens_d'] = disc_result.iloc[0]['cohens_d']\n",
        "        \n",
        "        summary_data.append(summary)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "print('\\nFeature summary:')\n",
        "print(summary_df)\n",
        "\n",
        "# Save summary\n",
        "summary_df.to_csv(reports_dir / 'feature_summary.csv', index=False)\n",
        "print(f'\\n✓ Saved feature summary to {reports_dir / \"feature_summary.csv\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook analyzed feature distributions:\n",
        "\n",
        "1. **Univariate Distributions**: Identified normal vs skewed features\n",
        "2. **Categorical Features**: Analyzed category distributions\n",
        "3. **Seasonal Patterns**: Examined temporal variation\n",
        "4. **Presence/Absence**: Compared distributions between classes\n",
        "5. **Statistical Tests**: Identified discriminative features\n",
        "6. **Normality Tests**: Flagged features needing transformation\n",
        "\n",
        "**Key Findings**:\n",
        "- Review `data/reports/feature_discrimination.csv` for predictive features\n",
        "- Review `data/reports/feature_summary.csv` for comprehensive statistics\n",
        "- Features with high |Cohen's d| are strong predictors\n",
        "\n",
        "**Next Steps**:\n",
        "- Proceed to Notebook 08 for spatial-temporal analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
