{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 10: Heuristic Validation\n",
    "\n",
    "## Purpose\n",
    "Validate that scoring heuristics (nutrition, security, etc.) align with actual elk behavior and are predictive of presence.\n",
    "\n",
    "## Key Questions\n",
    "- Which heuristic scores are available in the dataset?\n",
    "- Do heuristic scores correlate with their source features?\n",
    "- Are heuristics discriminative for elk presence?\n",
    "- Do composite scores outperform individual heuristics?\n",
    "- Should heuristics be kept as features or replaced with raw features?\n",
    "\n",
    "## Key Observations to Look For\n",
    "- **Score-Feature Correlation**: Heuristics should correlate with source features (>0.5)\n",
    "- **Discriminative Power**: AUC-ROC > 0.5 (better than random)\n",
    "- **Composite Performance**: Should composite score improve on individual scores?\n",
    "- **Weight Validation**: Do heuristic weights align with data-driven importance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Determine project root and output directories\n",
    "possible_roots = [\n",
    "    Path('.'),  # If running from project root\n",
    "    Path('..'),  # If running from notebooks directory\n",
    "    Path('../..'),  # If running from subdirectory\n",
    "]\n",
    "\n",
    "data_root = None\n",
    "for root in possible_roots:\n",
    "    if (root / 'data' / 'features').exists():\n",
    "        data_root = root / 'data'\n",
    "        break\n",
    "\n",
    "if data_root is None:\n",
    "    data_root = Path('../data')\n",
    "\n",
    "# Create output directories relative to project root\n",
    "figures_dir = data_root / 'figures'\n",
    "reports_dir = data_root / 'reports'\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'✓ Setup complete')\n",
    "print(f'  Output directory: {data_root.absolute()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Identify Heuristic Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from pathlib import Path\n",
    "\n",
    "# Try multiple possible paths\n",
    "possible_paths = [\n",
    "    Path('data/features/complete_context.csv'),  # From project root\n",
    "    Path('../data/features/complete_context.csv'),  # From notebooks directory\n",
    "    Path('../../data/features/complete_context.csv'),  # From subdirectory\n",
    "]\n",
    "\n",
    "data_path = None\n",
    "for path in possible_paths:\n",
    "    if path.exists():\n",
    "        data_path = path\n",
    "        break\n",
    "\n",
    "if data_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f'Data file not found. Tried: {[str(p) for p in possible_paths]}\\n'\n",
    "        f'Please run: python scripts/combine_feature_files.py\\n'\n",
    "        f'Or ensure you are running the notebook from the project root directory.'\n",
    "    )\n",
    "\n",
    "print(f'Loading data from: {data_path}')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Automatically detect heuristic score columns\n",
    "heuristic_cols = [col for col in df.columns if 'score' in col.lower()]\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'\\nFound {len(heuristic_cols)} heuristic score columns:')\n",
    "for col in heuristic_cols:\n",
    "    print(f'  - {col}')\n",
    "\n",
    "if len(heuristic_cols) == 0:\n",
    "    print('\\n⚠ No heuristic score columns found. Skipping heuristic validation.')\n",
    "    print('This notebook requires columns containing \"score\" in the name.')\n",
    "else:\n",
    "    # Detect presence column\n",
    "    presence_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in ['presence', 'target', 'label', 'is_presence', 'elk_present']:\n",
    "            presence_col = col\n",
    "            break\n",
    "    \n",
    "    print(f'\\nPresence column: {presence_col}')\n",
    "    \n",
    "    # Display heuristic score summary statistics\n",
    "    print('\\nHeuristic score statistics:')\n",
    "    print(df[heuristic_cols].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Heuristic Score Distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for all heuristic scores\n",
    "if len(heuristic_cols) > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = (len(heuristic_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for idx, col in enumerate(heuristic_cols):\n",
    "        ax = axes[idx]\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            # Histogram\n",
    "            ax.hist(data, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            \n",
    "            # Add mean/median lines\n",
    "            mean_val = data.mean()\n",
    "            median_val = data.median()\n",
    "            ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "            ax.axvline(median_val, color='blue', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "            \n",
    "            ax.set_xlabel(col, fontsize=10)\n",
    "            ax.set_ylabel('Frequency', fontsize=10)\n",
    "            ax.set_title(col, fontsize=11)\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(heuristic_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Heuristic Score Distributions', fontsize=16, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'heuristic_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print('✓ Saved heuristic distributions')\n",
    "    \n",
    "    # Compare presence vs absence if available\n",
    "    if presence_col:\n",
    "        print('\\n### Presence vs Absence Comparison:')\n",
    "        for col in heuristic_cols:\n",
    "            presence_mean = df[df[presence_col] == 1][col].mean()\n",
    "            absence_mean = df[df[presence_col] == 0][col].mean()\n",
    "            diff = presence_mean - absence_mean\n",
    "            print(f'  {col}: Presence={presence_mean:.3f}, Absence={absence_mean:.3f}, Diff={diff:.3f}')\n",
    "else:\n",
    "    print('⚠ No heuristic scores to analyze')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score vs Source Features Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate heuristic scores against their source features\n",
    "if len(heuristic_cols) > 0:\n",
    "    # Map heuristic names to likely source features\n",
    "    source_mapping = {}\n",
    "    for score_col in heuristic_cols:\n",
    "        score_lower = score_col.lower()\n",
    "        source_features = []\n",
    "        \n",
    "        # Try to identify source features based on heuristic name\n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if col == score_col:\n",
    "                continue\n",
    "            \n",
    "            # Nutrition score -> NDVI\n",
    "            if 'nutrition' in score_lower and 'ndvi' in col_lower:\n",
    "                source_features.append(col)\n",
    "            # Water score -> water_distance\n",
    "            elif 'water' in score_lower and 'water' in col_lower and 'distance' in col_lower:\n",
    "                source_features.append(col)\n",
    "            # Elevation score -> elevation\n",
    "            elif 'elevation' in score_lower and ('elev' in col_lower or 'altitude' in col_lower):\n",
    "                source_features.append(col)\n",
    "            # Security score -> security_habitat or road_distance\n",
    "            elif 'security' in score_lower and ('security' in col_lower or 'road' in col_lower):\n",
    "                source_features.append(col)\n",
    "            # Snow score -> snow_depth or snow_water_equiv\n",
    "            elif 'snow' in score_lower and 'snow' in col_lower:\n",
    "                source_features.append(col)\n",
    "        \n",
    "        if source_features:\n",
    "            source_mapping[score_col] = source_features\n",
    "    \n",
    "    # Create scatter plots for score vs source features\n",
    "    if len(source_mapping) > 0:\n",
    "        n_pairs = sum(len(sources) for sources in source_mapping.values())\n",
    "        n_cols = 3\n",
    "        n_rows = (n_pairs + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "        \n",
    "        plot_idx = 0\n",
    "        validation_results = []\n",
    "        \n",
    "        for score_col, source_features in source_mapping.items():\n",
    "            for source_col in source_features:\n",
    "                ax = axes[plot_idx]\n",
    "                \n",
    "                # Sample if too many points\n",
    "                sample_size = min(5000, len(df))\n",
    "                df_sample = df.sample(n=sample_size, random_state=42)\n",
    "                \n",
    "                # Scatter plot\n",
    "                ax.scatter(df_sample[source_col], df_sample[score_col], \n",
    "                          alpha=0.3, s=10, edgecolors='none')\n",
    "                \n",
    "                # Calculate correlation\n",
    "                corr = df[[source_col, score_col]].corr().iloc[0, 1]\n",
    "                \n",
    "                # Add regression line\n",
    "                z = np.polyfit(df_sample[source_col].dropna(), \n",
    "                              df_sample[score_col].dropna(), 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_line = np.linspace(df_sample[source_col].min(), \n",
    "                                    df_sample[source_col].max(), 100)\n",
    "                ax.plot(x_line, p(x_line), \"r--\", alpha=0.8, linewidth=2)\n",
    "                \n",
    "                ax.set_xlabel(source_col, fontsize=10)\n",
    "                ax.set_ylabel(score_col, fontsize=10)\n",
    "                ax.set_title(f'{score_col} vs {source_col}\\nr={corr:.3f}', fontsize=11)\n",
    "                ax.grid(alpha=0.3)\n",
    "                \n",
    "                validation_results.append({\n",
    "                    'heuristic': score_col,\n",
    "                    'source_feature': source_col,\n",
    "                    'correlation': corr,\n",
    "                    'status': 'PASS' if abs(corr) > 0.5 else 'FAIL'\n",
    "                })\n",
    "                \n",
    "                plot_idx += 1\n",
    "        \n",
    "        # Hide extra subplots\n",
    "        for idx in range(plot_idx, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.suptitle('Heuristic Score Validation (vs Source Features)', fontsize=16, y=1.00)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figures_dir / 'heuristic_validation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print('✓ Saved heuristic validation plots')\n",
    "        \n",
    "        # Print validation results\n",
    "        validation_df = pd.DataFrame(validation_results)\n",
    "        print('\\n### Validation Results:')\n",
    "        print(validation_df)\n",
    "        \n",
    "        failed = validation_df[validation_df['status'] == 'FAIL']\n",
    "        if len(failed) > 0:\n",
    "            print(f'\\n⚠ WARNING: {len(failed)} heuristic-source pairs have low correlation (<0.5):')\n",
    "            for _, row in failed.iterrows():\n",
    "                print(f\"  - {row['heuristic']} vs {row['source_feature']}: r={row['correlation']:.3f}\")\n",
    "        else:\n",
    "            print('\\n✓ All heuristic-source correlations meet threshold (>0.5)')\n",
    "    else:\n",
    "        print('⚠ Could not identify source features for heuristics')\n",
    "else:\n",
    "    print('⚠ No heuristic scores to validate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC-ROC for each heuristic\n",
    "if len(heuristic_cols) > 0 and presence_col:\n",
    "    auc_results = []\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for col in heuristic_cols:\n",
    "        # Remove NaN values\n",
    "        valid_mask = df[[col, presence_col]].notna().all(axis=1)\n",
    "        y_true = df.loc[valid_mask, presence_col]\n",
    "        y_scores = df.loc[valid_mask, col]\n",
    "        \n",
    "        if len(y_true) > 0 and len(y_scores) > 0:\n",
    "            # Calculate ROC curve\n",
    "            fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            auc_results.append({\n",
    "                'heuristic': col,\n",
    "                'auc': roc_auc\n",
    "            })\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            plt.plot(fpr, tpr, linewidth=2, label=f'{col} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    # Add diagonal line (random classifier)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.5)')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('ROC Curves for Heuristic Scores', fontsize=14, pad=20)\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'heuristic_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print('✓ Saved ROC curves')\n",
    "    \n",
    "    # Create AUC comparison bar chart\n",
    "    if len(auc_results) > 0:\n",
    "        auc_df = pd.DataFrame(auc_results).sort_values('auc', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        colors = ['green' if x > 0.5 else 'red' for x in auc_df['auc']]\n",
    "        plt.barh(range(len(auc_df)), auc_df['auc'], color=colors, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(0.5, color='red', linestyle='--', linewidth=2, label='Random (0.5)')\n",
    "        plt.yticks(range(len(auc_df)), auc_df['heuristic'])\n",
    "        plt.xlabel('AUC-ROC', fontsize=12)\n",
    "        plt.ylabel('Heuristic Score', fontsize=12)\n",
    "        plt.title('Heuristic Discriminative Power (AUC-ROC)', fontsize=14, pad=20)\n",
    "        plt.legend()\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figures_dir / 'heuristic_auc_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print('\\n### AUC-ROC Results (sorted by AUC):')\n",
    "        print(auc_df)\n",
    "        \n",
    "        # Flag poor performers\n",
    "        poor_performers = auc_df[auc_df['auc'] < 0.55]\n",
    "        if len(poor_performers) > 0:\n",
    "            print(f'\\n⚠ WARNING: {len(poor_performers)} heuristics perform no better than random (AUC < 0.55):')\n",
    "            for _, row in poor_performers.iterrows():\n",
    "                print(f\"  - {row['heuristic']}: AUC={row['auc']:.3f}\")\n",
    "        else:\n",
    "            print('\\n✓ All heuristics perform better than random')\n",
    "else:\n",
    "    print('⚠ Cannot calculate AUC without heuristic scores and presence column')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix for Top Heuristic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for best heuristic\n",
    "if len(heuristic_cols) > 0 and presence_col and len(auc_results) > 0:\n",
    "    # Get best heuristic\n",
    "    best_heuristic = auc_df.iloc[0]['heuristic']\n",
    "    best_auc = auc_df.iloc[0]['auc']\n",
    "    \n",
    "    print(f'Using best heuristic: {best_heuristic} (AUC={best_auc:.3f})')\n",
    "    \n",
    "    # Create binary classifier using median threshold\n",
    "    valid_mask = df[[best_heuristic, presence_col]].notna().all(axis=1)\n",
    "    y_true = df.loc[valid_mask, presence_col]\n",
    "    y_scores = df.loc[valid_mask, best_heuristic]\n",
    "    \n",
    "    threshold = y_scores.median()\n",
    "    y_pred = (y_scores > threshold).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    accuracy = (y_pred == y_true).mean()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "                xticklabels=['Absence', 'Presence'],\n",
    "                yticklabels=['Absence', 'Presence'])\n",
    "    plt.xlabel('Predicted', fontsize=12)\n",
    "    plt.ylabel('Actual', fontsize=12)\n",
    "    plt.title(f'Confusion Matrix: {best_heuristic}\\n'\n",
    "              f'Threshold={threshold:.3f}, Accuracy={accuracy:.3f}, F1={f1:.3f}', \n",
    "              fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'heuristic_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\n### Performance Metrics:')\n",
    "    print(f'  Accuracy: {accuracy:.3f}')\n",
    "    print(f'  Precision: {precision:.3f}')\n",
    "    print(f'  Recall: {recall:.3f}')\n",
    "    print(f'  F1-Score: {f1:.3f}')\n",
    "    print(f'  Baseline (always majority): {(y_true == y_true.mode()[0]).mean():.3f}')\n",
    "    \n",
    "    if accuracy > (y_true == y_true.mode()[0]).mean():\n",
    "        print('\\n✓ Heuristic outperforms majority class baseline')\n",
    "    else:\n",
    "        print('\\n⚠ Heuristic does not outperform majority class baseline')\n",
    "else:\n",
    "    print('⚠ Cannot create confusion matrix without heuristic scores and presence column')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Heuristic Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "if len(heuristic_cols) > 0:\n",
    "    recommendations = []\n",
    "    \n",
    "    # Compile all analysis results\n",
    "    if 'auc_df' in locals():\n",
    "        for _, row in auc_df.iterrows():\n",
    "            heuristic = row['heuristic']\n",
    "            auc_val = row['auc']\n",
    "            \n",
    "            rec = {\n",
    "                'heuristic': heuristic,\n",
    "                'auc': auc_val,\n",
    "                'recommendation': 'KEEP' if auc_val > 0.55 else 'DROP',\n",
    "                'reason': f'AUC={auc_val:.3f} {\"above\" if auc_val > 0.55 else \"below\"} 0.55 threshold'\n",
    "            }\n",
    "            \n",
    "            # Check validation if available\n",
    "            if 'validation_df' in locals():\n",
    "                validation = validation_df[validation_df['heuristic'] == heuristic]\n",
    "                if len(validation) > 0:\n",
    "                    low_corr = validation[validation['status'] == 'FAIL']\n",
    "                    if len(low_corr) > 0:\n",
    "                        rec['recommendation'] = 'REVIEW'\n",
    "                        rec['reason'] += f'; Low correlation with source features'\n",
    "            \n",
    "            recommendations.append(rec)\n",
    "    \n",
    "    if len(recommendations) > 0:\n",
    "        rec_df = pd.DataFrame(recommendations)\n",
    "        \n",
    "        print('### Heuristic Recommendations:')\n",
    "        print(rec_df)\n",
    "        \n",
    "        # Save recommendations\n",
    "        report = f'''# Heuristic Validation Recommendations\n",
    "\n",
    "Generated: {pd.Timestamp.now()}\n",
    "\n",
    "## Summary\n",
    "\n",
    "Total heuristics analyzed: {len(heuristic_cols)}\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "'''\n",
    "        \n",
    "        for _, rec in rec_df.iterrows():\n",
    "            report += f'''\n",
    "### {rec['heuristic']}\n",
    "- **AUC-ROC**: {rec['auc']:.3f}\n",
    "- **Recommendation**: {rec['recommendation']}\n",
    "- **Reason**: {rec['reason']}\n",
    "\n",
    "'''\n",
    "        \n",
    "        report += '''\n",
    "## General Guidelines\n",
    "\n",
    "1. **KEEP**: Heuristics with AUC > 0.55 and good source feature correlation\n",
    "2. **REVIEW**: Heuristics with low correlation to source features (may need recalibration)\n",
    "3. **DROP**: Heuristics with AUC < 0.55 (no better than random)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Consider keeping high-performing heuristics as features\n",
    "- Review and potentially recalibrate heuristics with low source correlation\n",
    "- Replace low-performing heuristics with raw features in modeling\n",
    "'''\n",
    "        \n",
    "        with open(reports_dir / 'heuristic_recommendations.md', 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print('\\n✓ Saved recommendations to data/reports/heuristic_recommendations.md')\n",
    "    else:\n",
    "        print('⚠ Could not generate recommendations')\n",
    "else:\n",
    "    print('⚠ No heuristics to recommend on')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook validated heuristic scores across multiple dimensions:\n",
    "\n",
    "1. **Score Distributions**: Examined distribution of each heuristic\n",
    "2. **Source Feature Validation**: Checked correlation with source features\n",
    "3. **Discriminative Power**: Calculated AUC-ROC for each heuristic\n",
    "4. **Confusion Matrix**: Evaluated best heuristic as binary classifier\n",
    "5. **Recommendations**: Generated actionable recommendations\n",
    "\n",
    "**Key Findings**:\n",
    "- Review `data/reports/heuristic_recommendations.md` for detailed recommendations\n",
    "- Heuristics with AUC > 0.55 are potentially useful features\n",
    "- Heuristics with low source correlation may need recalibration\n",
    "\n",
    "**Next Steps**:\n",
    "- Proceed to Notebook 11 for target variable analysis\n",
    "- Consider keeping high-performing heuristics as model features\n",
    "- Review and fix heuristics with validation issues\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
