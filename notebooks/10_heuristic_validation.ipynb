{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 10: Heuristic Validation\n",
    "\n",
    "## Purpose\n",
    "Validate that scoring heuristics (nutrition, security, etc.) align with actual elk behavior and are predictive of presence.\n",
    "\n",
    "## Key Questions\n",
    "- Which heuristic scores are available in the dataset?\n",
    "- Do heuristic scores correlate with their source features?\n",
    "- Are heuristics discriminative for elk presence?\n",
    "- Do composite scores outperform individual heuristics?\n",
    "- Should heuristics be kept as features or replaced with raw features?\n",
    "\n",
    "## Key Observations to Look For\n",
    "- **Score-Feature Correlation**: Heuristics should correlate with source features (>0.5)\n",
    "- **Discriminative Power**: AUC-ROC > 0.5 (better than random)\n",
    "- **Composite Performance**: Should composite score improve on individual scores?\n",
    "- **Weight Validation**: Do heuristic weights align with data-driven importance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Determine project root and output directories\n",
    "possible_roots = [\n",
    "    Path('.'),  # If running from project root\n",
    "    Path('..'),  # If running from notebooks directory\n",
    "    Path('../..'),  # If running from subdirectory\n",
    "]\n",
    "\n",
    "data_root = None\n",
    "for root in possible_roots:\n",
    "    if (root / 'data' / 'features').exists():\n",
    "        data_root = root / 'data'\n",
    "        break\n",
    "\n",
    "if data_root is None:\n",
    "    data_root = Path('../data')\n",
    "\n",
    "# Create output directories relative to project root\n",
    "figures_dir = data_root / 'figures'\n",
    "reports_dir = data_root / 'reports'\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'✓ Setup complete')\n",
    "print(f'  Output directory: {data_root.absolute()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Identify Heuristic Scores\n",
    "\n",
    "Heuristic scores are expert-derived features that combine multiple raw features into ecologically meaningful composite measures (e.g., nutrition score from NDVI, security score from cover and roads). This section identifies which heuristic scores are available in the dataset.\n",
    "\n",
    "### What to Look For\n",
    "- Confirm expected heuristic columns are present (nutrition_score, security_score, etc.)\n",
    "- Note range of scores (typically 0-1 or 0-100)\n",
    "- Check for missing values in heuristic columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from pathlib import Path\n",
    "\n",
    "# Try multiple possible paths\n",
    "possible_paths = [\n",
    "    Path('data/features/complete_context.csv'),  # From project root\n",
    "    Path('../data/features/complete_context.csv'),  # From notebooks directory\n",
    "    Path('../../data/features/complete_context.csv'),  # From subdirectory\n",
    "]\n",
    "\n",
    "data_path = None\n",
    "for path in possible_paths:\n",
    "    if path.exists():\n",
    "        data_path = path\n",
    "        break\n",
    "\n",
    "if data_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f'Data file not found. Tried: {[str(p) for p in possible_paths]}\\n'\n",
    "        f'Please run: python scripts/combine_feature_files.py\\n'\n",
    "        f'Or ensure you are running the notebook from the project root directory.'\n",
    "    )\n",
    "\n",
    "print(f'Loading data from: {data_path}')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Automatically detect heuristic score columns\n",
    "heuristic_cols = [col for col in df.columns if 'score' in col.lower()]\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'\\nFound {len(heuristic_cols)} heuristic score columns:')\n",
    "for col in heuristic_cols:\n",
    "    print(f'  - {col}')\n",
    "\n",
    "if len(heuristic_cols) == 0:\n",
    "    print('\\n⚠ No heuristic score columns found. Skipping heuristic validation.')\n",
    "    print('This notebook requires columns containing \"score\" in the name.')\n",
    "else:\n",
    "    # Detect presence column\n",
    "    presence_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in ['presence', 'target', 'label', 'is_presence', 'elk_present']:\n",
    "            presence_col = col\n",
    "            break\n",
    "    \n",
    "    print(f'\\nPresence column: {presence_col}')\n",
    "    \n",
    "    # Display heuristic score summary statistics\n",
    "    print('\\nHeuristic score statistics:')\n",
    "    print(df[heuristic_cols].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Heuristic Score Distributions\n",
    "\n",
    "Before validating heuristics against outcomes, we examine their distributions to ensure they vary appropriately and don't have pathological characteristics (constant values, extreme skew).\n",
    "\n",
    "### What This Code Does\n",
    "- Creates histograms for each heuristic score\n",
    "- Overlays mean and median reference lines\n",
    "- Compares score distributions between presence and absence classes\n",
    "\n",
    "### Interpreting Heuristic Distributions\n",
    "\n",
    "**Expected Characteristics:**\n",
    "- **Range**: Scores should span most of their theoretical range (e.g., 0-100)\n",
    "- **Variation**: Standard deviation should indicate meaningful spread\n",
    "- **Shape**: Roughly normal or uniform; avoid extreme spikes at bounds\n",
    "\n",
    "**Red Flags:**\n",
    "- **Constant value**: std ≈ 0 indicates broken heuristic calculation\n",
    "- **Spike at 0 or max**: May indicate ceiling/floor effects or missing data handling\n",
    "- **Bimodal**: May be appropriate (winter/summer) or indicate issues\n",
    "\n",
    "### Presence vs Absence Comparison\n",
    "\n",
    "**Expected Pattern:**\n",
    "- **Presence mean > Absence mean**: Heuristics should favor presence locations\n",
    "- **Substantial difference**: At least 5-10% of scale difference\n",
    "- **Overlapping distributions**: Some overlap is expected (not perfectly discriminative)\n",
    "\n",
    "### What to Look For\n",
    "- **Nutrition score**: Presence should have higher mean (better forage)\n",
    "- **Security score**: Presence should have higher mean (more cover)\n",
    "- **Water score**: Presence should have higher mean (closer to water)\n",
    "- **Overall/composite score**: Presence should be substantially higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for all heuristic scores\n",
    "if len(heuristic_cols) > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = (len(heuristic_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for idx, col in enumerate(heuristic_cols):\n",
    "        ax = axes[idx]\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            # Histogram\n",
    "            ax.hist(data, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            \n",
    "            # Add mean/median lines\n",
    "            mean_val = data.mean()\n",
    "            median_val = data.median()\n",
    "            ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "            ax.axvline(median_val, color='blue', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "            \n",
    "            ax.set_xlabel(col, fontsize=10)\n",
    "            ax.set_ylabel('Frequency', fontsize=10)\n",
    "            ax.set_title(col, fontsize=11)\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(heuristic_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Heuristic Score Distributions', fontsize=16, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'heuristic_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print('✓ Saved heuristic distributions')\n",
    "    \n",
    "    # Compare presence vs absence if available\n",
    "    if presence_col:\n",
    "        print('\\n### Presence vs Absence Comparison:')\n",
    "        for col in heuristic_cols:\n",
    "            presence_mean = df[df[presence_col] == 1][col].mean()\n",
    "            absence_mean = df[df[presence_col] == 0][col].mean()\n",
    "            diff = presence_mean - absence_mean\n",
    "            print(f'  {col}: Presence={presence_mean:.3f}, Absence={absence_mean:.3f}, Diff={diff:.3f}')\n",
    "else:\n",
    "    print('⚠ No heuristic scores to analyze')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score vs Source Features Validation\n",
    "\n",
    "Each heuristic score is computed from underlying \"source\" features (e.g., nutrition_score from NDVI). This section validates that scores correlate appropriately with their source features, confirming the heuristic calculation is working correctly.\n",
    "\n",
    "### What This Code Does\n",
    "- Maps each heuristic to its expected source features\n",
    "- Creates scatter plots of score vs source feature\n",
    "- Calculates correlation between score and source\n",
    "- Flags low correlations as validation failures\n",
    "\n",
    "### Interpreting Validation Results\n",
    "\n",
    "**Expected Correlations:**\n",
    "- **r > 0.7**: Strong correlation - heuristic accurately reflects source\n",
    "- **r 0.5-0.7**: Moderate correlation - heuristic combines multiple factors\n",
    "- **r < 0.5**: Weak correlation - may indicate calculation issues\n",
    "\n",
    "**Score-Source Mappings:**\n",
    "| Heuristic | Expected Source Features |\n",
    "|-----------|-------------------------|\n",
    "| nutrition_score | ndvi, vegetation quality |\n",
    "| security_score | canopy_cover, road_distance |\n",
    "| water_score | water_distance |\n",
    "| elevation_score | elevation (and slope) |\n",
    "| snow_score | snow_depth, snow_water_equiv |\n",
    "\n",
    "### What to Look For\n",
    "- **Linear relationship**: Score increases/decreases predictably with source\n",
    "- **Non-linear transformations**: Curved relationships indicate non-linear scoring functions\n",
    "- **Outliers**: Points far from the trend may indicate edge cases in scoring logic\n",
    "- **Ceiling effects**: Flat regions at score extremes\n",
    "\n",
    "### Validation Failures\n",
    "If score-source correlation is low (<0.5):\n",
    "1. **Check heuristic calculation**: May have bugs or incorrect weights\n",
    "2. **Check source data quality**: Source feature may have placeholder values\n",
    "3. **Check feature mapping**: Source feature name may have changed\n",
    "4. **Consider multi-factor scoring**: Score may legitimately combine multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate heuristic scores against their source features\n",
    "if len(heuristic_cols) > 0:\n",
    "    # Map heuristic names to likely source features\n",
    "    source_mapping = {}\n",
    "    for score_col in heuristic_cols:\n",
    "        score_lower = score_col.lower()\n",
    "        source_features = []\n",
    "        \n",
    "        # Try to identify source features based on heuristic name\n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if col == score_col:\n",
    "                continue\n",
    "            \n",
    "            # Nutrition score -> NDVI\n",
    "            if 'nutrition' in score_lower and 'ndvi' in col_lower:\n",
    "                source_features.append(col)\n",
    "            # Water score -> water_distance\n",
    "            elif 'water' in score_lower and 'water' in col_lower and 'distance' in col_lower:\n",
    "                source_features.append(col)\n",
    "            # Elevation score -> elevation\n",
    "            elif 'elevation' in score_lower and ('elev' in col_lower or 'altitude' in col_lower):\n",
    "                source_features.append(col)\n",
    "            # Security score -> security_habitat or road_distance\n",
    "            elif 'security' in score_lower and ('security' in col_lower or 'road' in col_lower):\n",
    "                source_features.append(col)\n",
    "            # Snow score -> snow_depth or snow_water_equiv\n",
    "            elif 'snow' in score_lower and 'snow' in col_lower:\n",
    "                source_features.append(col)\n",
    "        \n",
    "        if source_features:\n",
    "            source_mapping[score_col] = source_features\n",
    "    \n",
    "    # Create scatter plots for score vs source features\n",
    "    if len(source_mapping) > 0:\n",
    "        n_pairs = sum(len(sources) for sources in source_mapping.values())\n",
    "        n_cols = 3\n",
    "        n_rows = (n_pairs + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "        \n",
    "        plot_idx = 0\n",
    "        validation_results = []\n",
    "        \n",
    "        for score_col, source_features in source_mapping.items():\n",
    "            for source_col in source_features:\n",
    "                ax = axes[plot_idx]\n",
    "                \n",
    "                # Sample if too many points\n",
    "                sample_size = min(5000, len(df))\n",
    "                df_sample = df.sample(n=sample_size, random_state=42)\n",
    "                \n",
    "                # Scatter plot\n",
    "                ax.scatter(df_sample[source_col], df_sample[score_col], \n",
    "                          alpha=0.3, s=10, edgecolors='none')\n",
    "                \n",
    "                # Calculate correlation\n",
    "                corr = df[[source_col, score_col]].corr().iloc[0, 1]\n",
    "                \n",
    "                # Add regression line\n",
    "                z = np.polyfit(df_sample[source_col].dropna(), \n",
    "                              df_sample[score_col].dropna(), 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_line = np.linspace(df_sample[source_col].min(), \n",
    "                                    df_sample[source_col].max(), 100)\n",
    "                ax.plot(x_line, p(x_line), \"r--\", alpha=0.8, linewidth=2)\n",
    "                \n",
    "                ax.set_xlabel(source_col, fontsize=10)\n",
    "                ax.set_ylabel(score_col, fontsize=10)\n",
    "                ax.set_title(f'{score_col} vs {source_col}\\nr={corr:.3f}', fontsize=11)\n",
    "                ax.grid(alpha=0.3)\n",
    "                \n",
    "                validation_results.append({\n",
    "                    'heuristic': score_col,\n",
    "                    'source_feature': source_col,\n",
    "                    'correlation': corr,\n",
    "                    'status': 'PASS' if abs(corr) > 0.5 else 'FAIL'\n",
    "                })\n",
    "                \n",
    "                plot_idx += 1\n",
    "        \n",
    "        # Hide extra subplots\n",
    "        for idx in range(plot_idx, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.suptitle('Heuristic Score Validation (vs Source Features)', fontsize=16, y=1.00)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figures_dir / 'heuristic_validation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print('✓ Saved heuristic validation plots')\n",
    "        \n",
    "        # Print validation results\n",
    "        validation_df = pd.DataFrame(validation_results)\n",
    "        print('\\n### Validation Results:')\n",
    "        print(validation_df)\n",
    "        \n",
    "        failed = validation_df[validation_df['status'] == 'FAIL']\n",
    "        if len(failed) > 0:\n",
    "            print(f'\\n⚠ WARNING: {len(failed)} heuristic-source pairs have low correlation (<0.5):')\n",
    "            for _, row in failed.iterrows():\n",
    "                print(f\"  - {row['heuristic']} vs {row['source_feature']}: r={row['correlation']:.3f}\")\n",
    "        else:\n",
    "            print('\\n✓ All heuristic-source correlations meet threshold (>0.5)')\n",
    "    else:\n",
    "        print('⚠ Could not identify source features for heuristics')\n",
    "else:\n",
    "    print('⚠ No heuristic scores to validate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Heuristic Discriminative Power (ROC Analysis)\n",
    "\n",
    "The ultimate test of a heuristic is whether it can discriminate between presence and absence. ROC (Receiver Operating Characteristic) curves and AUC (Area Under Curve) quantify this discriminative ability.\n",
    "\n",
    "### What This Code Does\n",
    "- Calculates ROC curves treating each heuristic as a classifier\n",
    "- Computes AUC-ROC for each heuristic\n",
    "- Compares heuristics against random baseline (AUC = 0.5)\n",
    "- Creates a bar chart comparing AUC across heuristics\n",
    "\n",
    "### Understanding ROC Curves\n",
    "\n",
    "**ROC Curve Interpretation:**\n",
    "- **X-axis**: False Positive Rate (1 - Specificity)\n",
    "- **Y-axis**: True Positive Rate (Sensitivity/Recall)\n",
    "- **Diagonal line**: Random classifier (no discrimination)\n",
    "- **Curve above diagonal**: Better than random\n",
    "- **Perfect classifier**: Goes to top-left corner (AUC = 1.0)\n",
    "\n",
    "**AUC Interpretation:**\n",
    "- **AUC = 0.5**: No better than random guessing\n",
    "- **AUC 0.5-0.6**: Poor discrimination\n",
    "- **AUC 0.6-0.7**: Acceptable discrimination\n",
    "- **AUC 0.7-0.8**: Good discrimination\n",
    "- **AUC > 0.8**: Excellent discrimination\n",
    "\n",
    "### What to Look For\n",
    "- **All heuristics above diagonal**: Basic validation that they work\n",
    "- **AUC ranking**: Which heuristics are most discriminative?\n",
    "- **AUC < 0.55**: Heuristic is essentially useless - consider dropping\n",
    "- **Composite vs individual**: Does combining scores improve AUC?\n",
    "\n",
    "### Elk Ecology Context\n",
    "Expected heuristic ranking (roughly):\n",
    "1. **Composite/overall score**: Should be highest (combines all factors)\n",
    "2. **Nutrition score**: Forage is primary driver of elk distribution\n",
    "3. **Security score**: Cover and human avoidance are important\n",
    "4. **Water score**: Water proximity matters, especially in summer\n",
    "5. **Seasonal scores**: May vary by time of year\n",
    "\n",
    "### Implications for Modeling\n",
    "- **High AUC heuristics**: Keep as features or use directly for inference\n",
    "- **Low AUC heuristics**: Drop or investigate why they fail\n",
    "- **Similar AUC**: May indicate redundancy - keep most interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC-ROC for each heuristic\n",
    "if len(heuristic_cols) > 0 and presence_col:\n",
    "    auc_results = []\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for col in heuristic_cols:\n",
    "        # Remove NaN values\n",
    "        valid_mask = df[[col, presence_col]].notna().all(axis=1)\n",
    "        y_true = df.loc[valid_mask, presence_col]\n",
    "        y_scores = df.loc[valid_mask, col]\n",
    "        \n",
    "        if len(y_true) > 0 and len(y_scores) > 0:\n",
    "            # Calculate ROC curve\n",
    "            fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            auc_results.append({\n",
    "                'heuristic': col,\n",
    "                'auc': roc_auc\n",
    "            })\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            plt.plot(fpr, tpr, linewidth=2, label=f'{col} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    # Add diagonal line (random classifier)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.5)')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('ROC Curves for Heuristic Scores', fontsize=14, pad=20)\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'heuristic_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print('✓ Saved ROC curves')\n",
    "    \n",
    "    # Create AUC comparison bar chart\n",
    "    if len(auc_results) > 0:\n",
    "        auc_df = pd.DataFrame(auc_results).sort_values('auc', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        colors = ['green' if x > 0.5 else 'red' for x in auc_df['auc']]\n",
    "        plt.barh(range(len(auc_df)), auc_df['auc'], color=colors, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(0.5, color='red', linestyle='--', linewidth=2, label='Random (0.5)')\n",
    "        plt.yticks(range(len(auc_df)), auc_df['heuristic'])\n",
    "        plt.xlabel('AUC-ROC', fontsize=12)\n",
    "        plt.ylabel('Heuristic Score', fontsize=12)\n",
    "        plt.title('Heuristic Discriminative Power (AUC-ROC)', fontsize=14, pad=20)\n",
    "        plt.legend()\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figures_dir / 'heuristic_auc_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print('\\n### AUC-ROC Results (sorted by AUC):')\n",
    "        print(auc_df)\n",
    "        \n",
    "        # Flag poor performers\n",
    "        poor_performers = auc_df[auc_df['auc'] < 0.55]\n",
    "        if len(poor_performers) > 0:\n",
    "            print(f'\\n⚠ WARNING: {len(poor_performers)} heuristics perform no better than random (AUC < 0.55):')\n",
    "            for _, row in poor_performers.iterrows():\n",
    "                print(f\"  - {row['heuristic']}: AUC={row['auc']:.3f}\")\n",
    "        else:\n",
    "            print('\\n✓ All heuristics perform better than random')\n",
    "else:\n",
    "    print('⚠ Cannot calculate AUC without heuristic scores and presence column')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix for Top Heuristic\n",
    "\n",
    "To understand how well the best heuristic performs as an actual classifier, we convert it to binary predictions using a threshold and examine the confusion matrix.\n",
    "\n",
    "### What This Code Does\n",
    "- Selects the heuristic with highest AUC\n",
    "- Uses median score as classification threshold\n",
    "- Generates predictions (above median = predict presence)\n",
    "- Creates confusion matrix and calculates performance metrics\n",
    "\n",
    "### Interpreting the Confusion Matrix\n",
    "\n",
    "```\n",
    "                  Predicted\n",
    "                  Absence  Presence\n",
    "Actual  Absence    TN        FP\n",
    "        Presence   FN        TP\n",
    "```\n",
    "\n",
    "- **True Negatives (TN)**: Correctly predicted absence\n",
    "- **False Positives (FP)**: Predicted presence but was absence (Type I error)\n",
    "- **False Negatives (FN)**: Predicted absence but was presence (Type II error)\n",
    "- **True Positives (TP)**: Correctly predicted presence\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "- **Accuracy**: (TP + TN) / Total - overall correctness\n",
    "- **Precision**: TP / (TP + FP) - of predicted presence, how many correct?\n",
    "- **Recall**: TP / (TP + FN) - of actual presence, how many found?\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### What to Look For\n",
    "- **Accuracy vs baseline**: Must exceed majority class rate\n",
    "- **Precision/Recall balance**: Trade-off depends on application\n",
    "- **Diagonal dominance**: Most values on diagonal = good classifier\n",
    "- **Error patterns**: Where does the heuristic fail?\n",
    "\n",
    "### Ecological Interpretation of Errors\n",
    "- **False Positives**: Heuristic says good habitat, but no elk - may be unused suitable habitat\n",
    "- **False Negatives**: Elk present but heuristic says poor habitat - heuristic missing important factors\n",
    "\n",
    "### Threshold Considerations\n",
    "Using median as threshold is arbitrary. For inference applications:\n",
    "- **Higher threshold**: Fewer but more confident predictions\n",
    "- **Lower threshold**: More predictions but higher false positive rate\n",
    "- **Optimize for use case**: Conservation may want high recall; hunting management may want high precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for best heuristic\n",
    "if len(heuristic_cols) > 0 and presence_col and len(auc_results) > 0:\n",
    "    # Get best heuristic\n",
    "    best_heuristic = auc_df.iloc[0]['heuristic']\n",
    "    best_auc = auc_df.iloc[0]['auc']\n",
    "    \n",
    "    print(f'Using best heuristic: {best_heuristic} (AUC={best_auc:.3f})')\n",
    "    \n",
    "    # Create binary classifier using median threshold\n",
    "    valid_mask = df[[best_heuristic, presence_col]].notna().all(axis=1)\n",
    "    y_true = df.loc[valid_mask, presence_col]\n",
    "    y_scores = df.loc[valid_mask, best_heuristic]\n",
    "    \n",
    "    threshold = y_scores.median()\n",
    "    y_pred = (y_scores > threshold).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    accuracy = (y_pred == y_true).mean()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "                xticklabels=['Absence', 'Presence'],\n",
    "                yticklabels=['Absence', 'Presence'])\n",
    "    plt.xlabel('Predicted', fontsize=12)\n",
    "    plt.ylabel('Actual', fontsize=12)\n",
    "    plt.title(f'Confusion Matrix: {best_heuristic}\\n'\n",
    "              f'Threshold={threshold:.3f}, Accuracy={accuracy:.3f}, F1={f1:.3f}', \n",
    "              fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'heuristic_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\n### Performance Metrics:')\n",
    "    print(f'  Accuracy: {accuracy:.3f}')\n",
    "    print(f'  Precision: {precision:.3f}')\n",
    "    print(f'  Recall: {recall:.3f}')\n",
    "    print(f'  F1-Score: {f1:.3f}')\n",
    "    print(f'  Baseline (always majority): {(y_true == y_true.mode()[0]).mean():.3f}')\n",
    "    \n",
    "    if accuracy > (y_true == y_true.mode()[0]).mean():\n",
    "        print('\\n✓ Heuristic outperforms majority class baseline')\n",
    "    else:\n",
    "        print('\\n⚠ Heuristic does not outperform majority class baseline')\n",
    "else:\n",
    "    print('⚠ Cannot create confusion matrix without heuristic scores and presence column')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Heuristic Recommendations\n",
    "\n",
    "Based on all validation analyses, this section generates actionable recommendations for each heuristic: keep, drop, or review/recalibrate.\n",
    "\n",
    "### What This Code Does\n",
    "- Combines AUC and source correlation results\n",
    "- Applies decision rules to generate recommendations\n",
    "- Creates a summary report saved to `data/reports/heuristic_recommendations.md`\n",
    "\n",
    "### Recommendation Categories\n",
    "\n",
    "**KEEP (AUC > 0.55, good source correlation):**\n",
    "- Heuristic is working as intended\n",
    "- Include in feature set for modeling\n",
    "- May use directly for inference\n",
    "\n",
    "**REVIEW (mixed results):**\n",
    "- Low source correlation but reasonable AUC: Check calculation logic\n",
    "- Good source correlation but low AUC: Source feature may not be predictive\n",
    "- Investigate before including in models\n",
    "\n",
    "**DROP (AUC < 0.55):**\n",
    "- Heuristic provides no discrimination\n",
    "- Likely to add noise rather than signal\n",
    "- Consider replacing with raw features\n",
    "\n",
    "### How to Use Recommendations\n",
    "\n",
    "**For Model Training:**\n",
    "1. Include KEEP heuristics as features\n",
    "2. Investigate REVIEW heuristics before including\n",
    "3. Exclude DROP heuristics\n",
    "\n",
    "**For Heuristic Refinement:**\n",
    "1. Review weighting schemes for low-performing heuristics\n",
    "2. Consider adding missing factors\n",
    "3. Validate against ecological literature\n",
    "\n",
    "**For Inference Pipeline:**\n",
    "1. KEEP heuristics can be used for real-time scoring\n",
    "2. DROP heuristics should be removed from inference\n",
    "3. REVIEW heuristics need fixes before production use\n",
    "\n",
    "### Saved Output\n",
    "The detailed recommendations are saved to `data/reports/heuristic_recommendations.md` for reference during model development and heuristic refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "if len(heuristic_cols) > 0:\n",
    "    recommendations = []\n",
    "    \n",
    "    # Compile all analysis results\n",
    "    if 'auc_df' in locals():\n",
    "        for _, row in auc_df.iterrows():\n",
    "            heuristic = row['heuristic']\n",
    "            auc_val = row['auc']\n",
    "            \n",
    "            rec = {\n",
    "                'heuristic': heuristic,\n",
    "                'auc': auc_val,\n",
    "                'recommendation': 'KEEP' if auc_val > 0.55 else 'DROP',\n",
    "                'reason': f'AUC={auc_val:.3f} {\"above\" if auc_val > 0.55 else \"below\"} 0.55 threshold'\n",
    "            }\n",
    "            \n",
    "            # Check validation if available\n",
    "            if 'validation_df' in locals():\n",
    "                validation = validation_df[validation_df['heuristic'] == heuristic]\n",
    "                if len(validation) > 0:\n",
    "                    low_corr = validation[validation['status'] == 'FAIL']\n",
    "                    if len(low_corr) > 0:\n",
    "                        rec['recommendation'] = 'REVIEW'\n",
    "                        rec['reason'] += f'; Low correlation with source features'\n",
    "            \n",
    "            recommendations.append(rec)\n",
    "    \n",
    "    if len(recommendations) > 0:\n",
    "        rec_df = pd.DataFrame(recommendations)\n",
    "        \n",
    "        print('### Heuristic Recommendations:')\n",
    "        print(rec_df)\n",
    "        \n",
    "        # Save recommendations\n",
    "        report = f'''# Heuristic Validation Recommendations\n",
    "\n",
    "Generated: {pd.Timestamp.now()}\n",
    "\n",
    "## Summary\n",
    "\n",
    "Total heuristics analyzed: {len(heuristic_cols)}\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "'''\n",
    "        \n",
    "        for _, rec in rec_df.iterrows():\n",
    "            report += f'''\n",
    "### {rec['heuristic']}\n",
    "- **AUC-ROC**: {rec['auc']:.3f}\n",
    "- **Recommendation**: {rec['recommendation']}\n",
    "- **Reason**: {rec['reason']}\n",
    "\n",
    "'''\n",
    "        \n",
    "        report += '''\n",
    "## General Guidelines\n",
    "\n",
    "1. **KEEP**: Heuristics with AUC > 0.55 and good source feature correlation\n",
    "2. **REVIEW**: Heuristics with low correlation to source features (may need recalibration)\n",
    "3. **DROP**: Heuristics with AUC < 0.55 (no better than random)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Consider keeping high-performing heuristics as features\n",
    "- Review and potentially recalibrate heuristics with low source correlation\n",
    "- Replace low-performing heuristics with raw features in modeling\n",
    "'''\n",
    "        \n",
    "        with open(reports_dir / 'heuristic_recommendations.md', 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print('\\n✓ Saved recommendations to data/reports/heuristic_recommendations.md')\n",
    "    else:\n",
    "        print('⚠ Could not generate recommendations')\n",
    "else:\n",
    "    print('⚠ No heuristics to recommend on')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook validated heuristic scores across multiple dimensions:\n",
    "\n",
    "1. **Score Distributions**: Examined distribution of each heuristic\n",
    "2. **Source Feature Validation**: Checked correlation with source features\n",
    "3. **Discriminative Power**: Calculated AUC-ROC for each heuristic\n",
    "4. **Confusion Matrix**: Evaluated best heuristic as binary classifier\n",
    "5. **Recommendations**: Generated actionable recommendations\n",
    "\n",
    "**Key Findings**:\n",
    "- Review `data/reports/heuristic_recommendations.md` for detailed recommendations\n",
    "- Heuristics with AUC > 0.55 are potentially useful features\n",
    "- Heuristics with low source correlation may need recalibration\n",
    "\n",
    "**Next Steps**:\n",
    "- Proceed to Notebook 11 for target variable analysis\n",
    "- Consider keeping high-performing heuristics as model features\n",
    "- Review and fix heuristics with validation issues\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
