{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 9: Feature Correlations\n",
        "\n",
        "## Purpose\n",
        "Understand relationships between features and identify multicollinearity issues.\n",
        "\n",
        "## Key Questions\n",
        "- Which feature pairs are highly correlated?\n",
        "- Are there redundant features?\n",
        "- Which features are most correlated with the target?\n",
        "- Do correlations change seasonally?\n",
        "\n",
        "## Key Observations to Look For\n",
        "- **High Correlations**: |r| > 0.8 indicates potential redundancy\n",
        "- **VIF > 10**: Severe multicollinearity problem\n",
        "- **Target Correlation**: Identifies most predictive features\n",
        "- **Non-linear Relationships**: May need polynomial terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import pointbiserialr\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Determine project root and output directories\n",
        "possible_roots = [\n",
        "    Path('.'),  # If running from project root\n",
        "    Path('..'),  # If running from notebooks directory\n",
        "    Path('../..'),  # If running from subdirectory\n",
        "]\n",
        "\n",
        "data_root = None\n",
        "for root in possible_roots:\n",
        "    if (root / 'data' / 'features').exists():\n",
        "        data_root = root / 'data'\n",
        "        break\n",
        "\n",
        "if data_root is None:\n",
        "    data_root = Path('../data')\n",
        "\n",
        "# Create output directories relative to project root\n",
        "figures_dir = data_root / 'figures'\n",
        "reports_dir = data_root / 'reports'\n",
        "figures_dir.mkdir(parents=True, exist_ok=True)\n",
        "reports_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f'✓ Setup complete')\n",
        "print(f'  Output directory: {data_root.absolute()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('data/features/complete_context.csv')\n",
        "\n",
        "# Detect key columns\n",
        "timestamp_col = None\n",
        "presence_col = None\n",
        "\n",
        "for col in df.columns:\n",
        "    if any(x in col.lower() for x in ['timestamp', 'date', 'time']):\n",
        "        timestamp_col = col\n",
        "    if col.lower() in ['presence', 'target', 'label', 'is_presence']:\n",
        "        presence_col = col\n",
        "\n",
        "if timestamp_col:\n",
        "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce')\n",
        "    df['month'] = df[timestamp_col].dt.month\n",
        "\n",
        "print(f'Dataset shape: {df.shape}')\n",
        "print(f'Presence column: {presence_col}')\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f'Numeric columns: {len(numeric_cols)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Pearson correlation matrix\n",
        "corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "# Create large annotated heatmap\n",
        "plt.figure(figsize=(16, 14))\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    cmap='RdBu_r',\n",
        "    center=0,\n",
        "    vmin=-1,\n",
        "    vmax=1,\n",
        "    square=True,\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={'label': 'Pearson Correlation', 'shrink': 0.8}\n",
        ")\n",
        "plt.title('Feature Correlation Matrix', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "plt.yticks(rotation=0, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig(figures_dir / 'correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Saved correlation matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify high correlations\n",
        "high_corr_pairs = []\n",
        "\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "        corr_val = corr_matrix.iloc[i, j]\n",
        "        if abs(corr_val) > 0.8:\n",
        "            high_corr_pairs.append({\n",
        "                'feature1': corr_matrix.columns[i],\n",
        "                'feature2': corr_matrix.columns[j],\n",
        "                'correlation': corr_val\n",
        "            })\n",
        "\n",
        "if len(high_corr_pairs) > 0:\n",
        "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('correlation', key=abs, ascending=False)\n",
        "    print(f'\\n⚠ Found {len(high_corr_pairs)} feature pairs with |r| > 0.8:')\n",
        "    print(high_corr_df)\n",
        "    \n",
        "    print('\\nThese features may be redundant. Consider:')\n",
        "    print('- Dropping one feature from each pair')\n",
        "    print('- Using PCA for dimensionality reduction')\n",
        "    print('- Using regularization (L1/L2) in modeling')\n",
        "else:\n",
        "    print('\\n✓ No feature pairs with |r| > 0.8 (no severe multicollinearity)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Key Feature Pair Scatter Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create scatter plots for most correlated pairs\n",
        "if len(high_corr_pairs) > 0:\n",
        "    top_pairs = high_corr_df.head(6)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, (_, row) in enumerate(top_pairs.iterrows()):\n",
        "        if idx >= 6:\n",
        "            break\n",
        "        \n",
        "        ax = axes[idx]\n",
        "        feat1 = row['feature1']\n",
        "        feat2 = row['feature2']\n",
        "        corr = row['correlation']\n",
        "        \n",
        "        # Sample if too many points\n",
        "        plot_df = df[[feat1, feat2]].dropna()\n",
        "        if len(plot_df) > 5000:\n",
        "            plot_df = plot_df.sample(n=5000, random_state=42)\n",
        "        \n",
        "        ax.scatter(plot_df[feat1], plot_df[feat2], alpha=0.3, s=10)\n",
        "        \n",
        "        # Add regression line\n",
        "        z = np.polyfit(plot_df[feat1], plot_df[feat2], 1)\n",
        "        p = np.poly1d(z)\n",
        "        x_line = np.linspace(plot_df[feat1].min(), plot_df[feat1].max(), 100)\n",
        "        ax.plot(x_line, p(x_line), 'r-', linewidth=2, alpha=0.7)\n",
        "        \n",
        "        ax.set_xlabel(feat1, fontsize=9)\n",
        "        ax.set_ylabel(feat2, fontsize=9)\n",
        "        ax.set_title(f'{feat1} vs {feat2}\\nr = {corr:.3f}', fontsize=10)\n",
        "        ax.grid(alpha=0.3)\n",
        "    \n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(top_pairs), 6):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle('Key Feature Pair Correlations', fontsize=16, y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'key_correlations.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('✓ Saved key correlation scatter plots')\n",
        "else:\n",
        "    print('No high correlations to plot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Seasonal Correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlation matrices by season\n",
        "if timestamp_col and 'month' in df.columns:\n",
        "    # Define seasons\n",
        "    seasons = {\n",
        "        'Winter': [12, 1, 2],\n",
        "        'Spring': [3, 4, 5],\n",
        "        'Summer': [6, 7, 8],\n",
        "        'Fall': [9, 10, 11]\n",
        "    }\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, (season_name, months) in enumerate(seasons.items()):\n",
        "        season_df = df[df['month'].isin(months)]\n",
        "        season_corr = season_df[numeric_cols].corr()\n",
        "        \n",
        "        ax = axes[idx]\n",
        "        sns.heatmap(\n",
        "            season_corr,\n",
        "            cmap='RdBu_r',\n",
        "            center=0,\n",
        "            vmin=-1,\n",
        "            vmax=1,\n",
        "            square=True,\n",
        "            cbar_kws={'label': 'Correlation'},\n",
        "            ax=ax,\n",
        "            xticklabels=False,\n",
        "            yticklabels=False\n",
        "        )\n",
        "        ax.set_title(f'{season_name} (n={len(season_df):,})', fontsize=13)\n",
        "    \n",
        "    plt.suptitle('Seasonal Correlation Matrices', fontsize=16, y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'seasonal_correlations.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('✓ Saved seasonal correlation matrices')\n",
        "else:\n",
        "    print('⚠ Cannot analyze seasonal correlations without timestamp column')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Multicollinearity Detection (VIF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Variance Inflation Factor\n",
        "try:\n",
        "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "    \n",
        "    # Select features for VIF (exclude target, timestamps, etc.)\n",
        "    vif_features = [col for col in numeric_cols \n",
        "                   if col not in [presence_col, 'month', 'year']][:20]  # Limit for performance\n",
        "    \n",
        "    vif_data = df[vif_features].dropna()\n",
        "    \n",
        "    if len(vif_data) > 0:\n",
        "        vif_results = []\n",
        "        \n",
        "        for i, col in enumerate(vif_features):\n",
        "            try:\n",
        "                vif = variance_inflation_factor(vif_data.values, i)\n",
        "                vif_results.append({'feature': col, 'VIF': vif})\n",
        "            except:\n",
        "                vif_results.append({'feature': col, 'VIF': np.nan})\n",
        "        \n",
        "        vif_df = pd.DataFrame(vif_results).sort_values('VIF', ascending=False)\n",
        "        \n",
        "        print('\\nVariance Inflation Factors:')\n",
        "        print(vif_df)\n",
        "        \n",
        "        # Plot VIF\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.barh(range(len(vif_df)), vif_df['VIF'], color='steelblue', alpha=0.7)\n",
        "        plt.axvline(10, color='red', linestyle='--', linewidth=2, label='VIF=10 threshold')\n",
        "        plt.yticks(range(len(vif_df)), vif_df['feature'])\n",
        "        plt.xlabel('VIF', fontsize=12)\n",
        "        plt.ylabel('Feature', fontsize=12)\n",
        "        plt.title('Variance Inflation Factors (VIF > 10 indicates multicollinearity)', fontsize=14, pad=20)\n",
        "        plt.legend()\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(figures_dir / 'vif_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print('\\n✓ Saved VIF analysis')\n",
        "        \n",
        "        # Flag high VIF\n",
        "        high_vif = vif_df[vif_df['VIF'] > 10]\n",
        "        if len(high_vif) > 0:\n",
        "            print(f'\\n⚠ WARNING: {len(high_vif)} features with VIF > 10:')\n",
        "            for _, row in high_vif.iterrows():\n",
        "                print(f\"  - {row['feature']}: VIF = {row['VIF']:.2f}\")\n",
        "        else:\n",
        "            print('\\n✓ No features with VIF > 10')\n",
        "    else:\n",
        "        print('⚠ Insufficient data for VIF calculation')\n",
        "        \n",
        "except ImportError:\n",
        "    print('⚠ statsmodels not available for VIF calculation')\n",
        "except Exception as e:\n",
        "    print(f'⚠ Could not calculate VIF: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Target Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlation with target variable\n",
        "if presence_col:\n",
        "    target_corr = []\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        if col != presence_col:\n",
        "            # Use point-biserial correlation for binary target\n",
        "            data = df[[col, presence_col]].dropna()\n",
        "            \n",
        "            if len(data) > 0:\n",
        "                try:\n",
        "                    corr, p_value = pointbiserialr(data[presence_col], data[col])\n",
        "                    target_corr.append({\n",
        "                        'feature': col,\n",
        "                        'correlation': corr,\n",
        "                        'p_value': p_value,\n",
        "                        'abs_correlation': abs(corr)\n",
        "                    })\n",
        "                except:\n",
        "                    pass\n",
        "    \n",
        "    if len(target_corr) > 0:\n",
        "        target_corr_df = pd.DataFrame(target_corr).sort_values('abs_correlation', ascending=False)\n",
        "        \n",
        "        print('\\nFeature correlation with target:')\n",
        "        print(target_corr_df.head(20))\n",
        "        \n",
        "        # Plot top correlations\n",
        "        top_n = min(15, len(target_corr_df))\n",
        "        top_corr = target_corr_df.head(top_n)\n",
        "        \n",
        "        plt.figure(figsize=(12, 8))\n",
        "        colors = ['green' if x > 0 else 'red' for x in top_corr['correlation']]\n",
        "        plt.barh(range(len(top_corr)), top_corr['correlation'], color=colors, alpha=0.7)\n",
        "        plt.yticks(range(len(top_corr)), top_corr['feature'])\n",
        "        plt.xlabel('Point-Biserial Correlation with Target', fontsize=12)\n",
        "        plt.ylabel('Feature', fontsize=12)\n",
        "        plt.title(f'Top {top_n} Features by Target Correlation', fontsize=14, pad=20)\n",
        "        plt.axvline(0, color='black', linewidth=1)\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(figures_dir / 'target_correlations.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print('\\n✓ Saved target correlation plot')\n",
        "        \n",
        "        # Identify top predictive features\n",
        "        print(f'\\nTop 10 most correlated features with target:')\n",
        "        for _, row in target_corr_df.head(10).iterrows():\n",
        "            print(f\"  - {row['feature']}: r = {row['correlation']:.3f}, p = {row['p_value']:.2e}\")\n",
        "    else:\n",
        "        print('⚠ Could not calculate target correlations')\n",
        "else:\n",
        "    print('⚠ Cannot calculate target correlation without presence column')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Non-linear Relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine non-linear relationships with target\n",
        "if presence_col and len(target_corr) > 0:\n",
        "    top_features = target_corr_df.head(6)['feature'].tolist()\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, feat in enumerate(top_features):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Sample data\n",
        "        plot_df = df[[feat, presence_col]].dropna()\n",
        "        if len(plot_df) > 5000:\n",
        "            plot_df = plot_df.sample(n=5000, random_state=42)\n",
        "        \n",
        "        # Scatter plot with jitter on y-axis\n",
        "        jitter = np.random.normal(0, 0.02, len(plot_df))\n",
        "        ax.scatter(plot_df[feat], plot_df[presence_col] + jitter, alpha=0.1, s=5)\n",
        "        \n",
        "        # Add LOESS smoothing\n",
        "        try:\n",
        "            from scipy.interpolate import UnivariateSpline\n",
        "            \n",
        "            # Sort by feature value\n",
        "            sorted_df = plot_df.sort_values(feat)\n",
        "            \n",
        "            # Bin and calculate mean presence rate\n",
        "            n_bins = 20\n",
        "            bins = pd.qcut(sorted_df[feat], n_bins, duplicates='drop')\n",
        "            binned = sorted_df.groupby(bins)[presence_col].mean()\n",
        "            bin_centers = sorted_df.groupby(bins)[feat].mean()\n",
        "            \n",
        "            ax.plot(bin_centers, binned, 'r-', linewidth=3, label='Binned mean')\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        ax.set_xlabel(feat, fontsize=10)\n",
        "        ax.set_ylabel('Presence', fontsize=10)\n",
        "        ax.set_title(f'{feat}', fontsize=11)\n",
        "        ax.set_ylim(-0.1, 1.1)\n",
        "        ax.legend()\n",
        "        ax.grid(alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('Non-linear Relationships with Target', fontsize=16, y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'nonlinear_relationships.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('✓ Saved non-linear relationship plots')\n",
        "else:\n",
        "    print('⚠ Cannot analyze non-linear relationships')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Correlation Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive correlation summary\n",
        "summary = {\n",
        "    'high_correlations': len(high_corr_pairs) if len(high_corr_pairs) > 0 else 0,\n",
        "    'features_analyzed': len(numeric_cols)\n",
        "}\n",
        "\n",
        "if presence_col and len(target_corr) > 0:\n",
        "    summary['top_predictive_features'] = target_corr_df.head(10)['feature'].tolist()\n",
        "    summary['strongest_correlation'] = target_corr_df.iloc[0]['correlation']\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('CORRELATION ANALYSIS SUMMARY')\n",
        "print('='*70)\n",
        "print(f\"\\nFeatures analyzed: {summary['features_analyzed']}\")\n",
        "print(f\"High correlation pairs (|r|>0.8): {summary['high_correlations']}\")\n",
        "\n",
        "if 'top_predictive_features' in summary:\n",
        "    print(f\"\\nTop 10 predictive features:\")\n",
        "    for i, feat in enumerate(summary['top_predictive_features'], 1):\n",
        "        corr_val = target_corr_df[target_corr_df['feature']==feat].iloc[0]['correlation']\n",
        "        print(f\"  {i:2d}. {feat}: r = {corr_val:.3f}\")\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "\n",
        "# Save correlation analysis report\n",
        "if len(high_corr_pairs) > 0:\n",
        "    high_corr_df.to_csv(reports_dir / 'correlation_analysis.csv', index=False)\n",
        "    print(f'\\n✓ Saved correlation analysis to {reports_dir / \"correlation_analysis.csv\"}')\n",
        "\n",
        "if presence_col and len(target_corr) > 0:\n",
        "    target_corr_df.to_csv(reports_dir / 'target_correlations.csv', index=False)\n",
        "    print(f'✓ Saved target correlations to {reports_dir / \"target_correlations.csv\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook analyzed feature correlations:\n",
        "\n",
        "1. **Correlation Matrix**: Identified highly correlated feature pairs\n",
        "2. **Key Pairs**: Visualized strongest correlations\n",
        "3. **Seasonal Correlations**: Examined how relationships change by season\n",
        "4. **Multicollinearity (VIF)**: Flagged features with VIF > 10\n",
        "5. **Target Correlation**: Ranked features by predictive power\n",
        "6. **Non-linear Relationships**: Identified features needing polynomial terms\n",
        "\n",
        "**Key Findings**:\n",
        "- Review `data/reports/correlation_analysis.csv` for redundant features\n",
        "- Review `data/reports/target_correlations.csv` for feature selection\n",
        "- Consider dropping one feature from highly correlated pairs\n",
        "- Use regularization to handle multicollinearity\n",
        "\n",
        "**Recommendations**:\n",
        "- Features with VIF > 10 should be dropped or combined\n",
        "- Top correlated features are good candidates for modeling\n",
        "- Non-linear relationships may benefit from feature engineering\n",
        "\n",
        "**Next Steps**:\n",
        "- Proceed to Notebook 10 for heuristic validation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
