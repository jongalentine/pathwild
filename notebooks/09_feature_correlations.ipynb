{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 9: Feature Correlations\n",
    "\n",
    "## Purpose\n",
    "Understand relationships between features and identify multicollinearity issues.\n",
    "\n",
    "## Key Questions\n",
    "- Which feature pairs are highly correlated?\n",
    "- Are there redundant features?\n",
    "- Which features are most correlated with the target?\n",
    "- Do correlations change seasonally?\n",
    "\n",
    "## Key Observations to Look For\n",
    "- **High Correlations**: |r| > 0.8 indicates potential redundancy\n",
    "- **VIF > 10**: Severe multicollinearity problem\n",
    "- **Target Correlation**: Identifies most predictive features\n",
    "- **Non-linear Relationships**: May need polynomial terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import pointbiserialr\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Determine project root and output directories\n",
    "possible_roots = [\n",
    "    Path('.'),  # If running from project root\n",
    "    Path('..'),  # If running from notebooks directory\n",
    "    Path('../..'),  # If running from subdirectory\n",
    "]\n",
    "\n",
    "data_root = None\n",
    "for root in possible_roots:\n",
    "    if (root / 'data' / 'features').exists():\n",
    "        data_root = root / 'data'\n",
    "        break\n",
    "\n",
    "if data_root is None:\n",
    "    data_root = Path('../data')\n",
    "\n",
    "# Create output directories relative to project root\n",
    "figures_dir = data_root / 'figures'\n",
    "reports_dir = data_root / 'reports'\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'✓ Setup complete')\n",
    "print(f'  Output directory: {data_root.absolute()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Load Data\n\nThis section loads the feature dataset and identifies numeric columns for correlation analysis. Understanding feature relationships is critical for feature selection and avoiding multicollinearity issues in modeling."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/features/complete_context.csv')\n",
    "\n",
    "# Detect key columns\n",
    "timestamp_col = None\n",
    "presence_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if any(x in col.lower() for x in ['timestamp', 'date', 'time']):\n",
    "        timestamp_col = col\n",
    "    if col.lower() in ['presence', 'target', 'label', 'is_presence']:\n",
    "        presence_col = col\n",
    "\n",
    "if timestamp_col:\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce')\n",
    "    df['month'] = df[timestamp_col].dt.month\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Presence column: {presence_col}')\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f'Numeric columns: {len(numeric_cols)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Correlation Matrix\n\nThe correlation matrix shows pairwise Pearson correlations between all numeric features. High correlations (|r| > 0.8) indicate potential redundancy where one feature can be predicted from another.\n\n### What This Code Does\n- Calculates Pearson correlation coefficient for all feature pairs\n- Creates a heatmap visualization with color-coded correlations\n- Identifies highly correlated feature pairs (|r| > 0.8)\n\n### Interpreting the Correlation Heatmap\n\n**Color Scale:**\n- **Dark red (r = -1.0)**: Perfect negative correlation - as one increases, other decreases\n- **White (r = 0.0)**: No linear relationship\n- **Dark blue (r = +1.0)**: Perfect positive correlation - both increase together\n\n**Matrix Structure:**\n- **Diagonal**: Always +1.0 (feature correlated with itself)\n- **Symmetric**: Upper and lower triangles are mirrors\n- **Blocks of color**: Groups of related features (e.g., all snow features)\n\n### What to Look For\n- **Dark off-diagonal squares**: High correlations requiring attention\n- **Feature clusters**: Groups of correlated features (natural groupings)\n- **Unexpected correlations**: May indicate data issues or interesting relationships\n- **White rows/columns**: Features uncorrelated with others (often valuable)\n\n### Expected High Correlations in Elk Data\n- **Temperature features**: temp_max, temp_min, temp_mean highly correlated\n- **Snow features**: snow_depth and snow_water_equiv highly correlated\n- **Coordinate features**: lat/lon may correlate with elevation\n- **Derived features**: Encoded features (cos/sin) with original values\n\n### Implications for Modeling\n- **Linear models**: High multicollinearity causes unstable coefficients\n- **Tree-based models**: Less affected but redundancy wastes computation\n- **Feature selection**: Consider keeping only one from highly correlated pairs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Pearson correlation matrix\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Create large annotated heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'label': 'Pearson Correlation', 'shrink': 0.8}\n",
    ")\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, pad=20)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('✓ Saved correlation matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify high correlations\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'feature1': corr_matrix.columns[i],\n",
    "                'feature2': corr_matrix.columns[j],\n",
    "                'correlation': corr_val\n",
    "            })\n",
    "\n",
    "if len(high_corr_pairs) > 0:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('correlation', key=abs, ascending=False)\n",
    "    print(f'\\n⚠ Found {len(high_corr_pairs)} feature pairs with |r| > 0.8:')\n",
    "    print(high_corr_df)\n",
    "    \n",
    "    print('\\nThese features may be redundant. Consider:')\n",
    "    print('- Dropping one feature from each pair')\n",
    "    print('- Using PCA for dimensionality reduction')\n",
    "    print('- Using regularization (L1/L2) in modeling')\n",
    "else:\n",
    "    print('\\n✓ No feature pairs with |r| > 0.8 (no severe multicollinearity)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Key Feature Pair Scatter Plots\n\nScatter plots visualize the relationships between highly correlated feature pairs identified above. This helps verify that correlations are real (not driven by outliers) and understand the nature of relationships.\n\n### What This Code Does\n- Creates scatter plots for the top 6 most correlated feature pairs\n- Adds regression lines to show linear relationship\n- Displays correlation coefficient in title\n\n### Interpreting Scatter Plots\n\n**Pattern Types:**\n- **Tight linear band**: Strong linear correlation, features are redundant\n- **Scattered cloud with trend**: Moderate correlation, some independent information\n- **Curved relationship**: Non-linear correlation (Pearson may underestimate)\n- **Clusters**: Distinct groups may indicate categorical variable influence\n\n**Regression Line:**\n- **Slope direction**: Positive slope = positive correlation\n- **Points near line**: Tight fit confirms strong correlation\n- **Points far from line**: Relationship is noisy or non-linear\n\n### What to Look For\n- **Outliers driving correlation**: A few extreme points can inflate r\n- **Heteroscedasticity**: Spread changing along x-axis indicates non-constant variance\n- **Non-linear patterns**: Consider polynomial features or transformations\n- **Discrete bands**: May indicate categorical underlying variable\n\n### Decision Points for Redundant Features\nWhen |r| > 0.9, decide which feature to keep based on:\n1. **Interpretability**: Which is easier to explain?\n2. **Missing data**: Which has fewer missing values?\n3. **Target correlation**: Which correlates better with presence/absence?\n4. **Domain importance**: Which is more ecologically meaningful?\n\n### Example Decisions\n- **temp_max vs temp_mean**: Keep temp_mean (more representative)\n- **snow_depth vs snow_water_equiv**: Keep snow_water_equiv (more relevant to movement)\n- **latitude vs longitude**: Keep both (different information)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for most correlated pairs\n",
    "if len(high_corr_pairs) > 0:\n",
    "    top_pairs = high_corr_df.head(6)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (_, row) in enumerate(top_pairs.iterrows()):\n",
    "        if idx >= 6:\n",
    "            break\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        feat1 = row['feature1']\n",
    "        feat2 = row['feature2']\n",
    "        corr = row['correlation']\n",
    "        \n",
    "        # Sample if too many points\n",
    "        plot_df = df[[feat1, feat2]].dropna()\n",
    "        if len(plot_df) > 5000:\n",
    "            plot_df = plot_df.sample(n=5000, random_state=42)\n",
    "        \n",
    "        ax.scatter(plot_df[feat1], plot_df[feat2], alpha=0.3, s=10)\n",
    "        \n",
    "        # Add regression line\n",
    "        z = np.polyfit(plot_df[feat1], plot_df[feat2], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(plot_df[feat1].min(), plot_df[feat1].max(), 100)\n",
    "        ax.plot(x_line, p(x_line), 'r-', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel(feat1, fontsize=9)\n",
    "        ax.set_ylabel(feat2, fontsize=9)\n",
    "        ax.set_title(f'{feat1} vs {feat2}\\nr = {corr:.3f}', fontsize=10)\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(top_pairs), 6):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Key Feature Pair Correlations', fontsize=16, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'key_correlations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print('✓ Saved key correlation scatter plots')\n",
    "else:\n",
    "    print('No high correlations to plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Seasonal Correlations\n\nFeature relationships may change across seasons. For example, elevation and temperature might be more strongly correlated in summer than winter. This section examines how the correlation structure varies by season.\n\n### What This Code Does\n- Splits data into four seasons (Winter, Spring, Summer, Fall)\n- Calculates separate correlation matrices for each season\n- Displays four heatmaps side-by-side for comparison\n\n### Interpreting Seasonal Correlation Patterns\n\n**Stable Correlations (same across seasons):**\n- Indicates fundamental physical/ecological relationships\n- Safe to use in models regardless of season\n- Example: elevation-temperature relationship (lapse rate)\n\n**Variable Correlations (change by season):**\n- Relationships depend on seasonal context\n- May need season-specific models or interaction terms\n- Example: snow-elevation correlation (strong in winter, weak in summer)\n\n### What to Look For\n- **Winter vs Summer differences**: Most dramatic seasonal contrast\n- **Spring/Fall transitions**: May show intermediate patterns\n- **Disappearing correlations**: Features related only in certain seasons\n- **Sign changes**: Rare but important - relationship reverses by season\n\n### Elk Ecology Context\nSeasonal correlation changes reflect elk behavioral shifts:\n- **Winter**: Strong correlation between snow and movement constraints\n- **Spring**: NDVI-elevation correlation strengthens (green wave)\n- **Summer**: More dispersed, weaker spatial correlations\n- **Fall**: Rut behavior may change typical correlations\n\n### Implications for Modeling\n- **Single model**: May miss seasonal nuances\n- **Seasonal models**: Better fit but need more data per season\n- **Interaction terms**: Include season × feature interactions\n- **Ensemble**: Combine seasonal sub-models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrices by season\n",
    "if timestamp_col and 'month' in df.columns:\n",
    "    # Define seasons\n",
    "    seasons = {\n",
    "        'Winter': [12, 1, 2],\n",
    "        'Spring': [3, 4, 5],\n",
    "        'Summer': [6, 7, 8],\n",
    "        'Fall': [9, 10, 11]\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (season_name, months) in enumerate(seasons.items()):\n",
    "        season_df = df[df['month'].isin(months)]\n",
    "        season_corr = season_df[numeric_cols].corr()\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        sns.heatmap(\n",
    "            season_corr,\n",
    "            cmap='RdBu_r',\n",
    "            center=0,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Correlation'},\n",
    "            ax=ax,\n",
    "            xticklabels=False,\n",
    "            yticklabels=False\n",
    "        )\n",
    "        ax.set_title(f'{season_name} (n={len(season_df):,})', fontsize=13)\n",
    "    \n",
    "    plt.suptitle('Seasonal Correlation Matrices', fontsize=16, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'seasonal_correlations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print('✓ Saved seasonal correlation matrices')\n",
    "else:\n",
    "    print('⚠ Cannot analyze seasonal correlations without timestamp column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Multicollinearity Detection (VIF)\n\nVariance Inflation Factor (VIF) measures how much each feature's variance is inflated due to correlations with other features. Unlike pairwise correlation, VIF detects multicollinearity involving multiple features simultaneously.\n\n### What This Code Does\n- Calculates VIF for each numeric feature\n- Creates a horizontal bar chart for easy comparison\n- Flags features with VIF > 10 (severe multicollinearity)\n\n### Understanding VIF\n\n**VIF Calculation:**\nVIF = 1 / (1 - R²), where R² is from regressing feature on all other features\n\n**VIF Interpretation:**\n- **VIF = 1.0**: No correlation with other features (ideal)\n- **VIF 1-5**: Low to moderate correlation (acceptable)\n- **VIF 5-10**: High correlation (concerning for some models)\n- **VIF > 10**: Severe multicollinearity (problematic)\n- **VIF = ∞**: Perfect multicollinearity (feature is linear combination of others)\n\n### What to Look For\n- **Features with VIF > 10**: Primary candidates for removal\n- **Cluster of high VIF**: Related features all inflated together\n- **Single high VIF feature**: May be combining information from multiple sources\n- **Geographic coordinates**: Often have high VIF due to elevation correlation\n\n### How High VIF Affects Modeling\n\n**Linear Regression:**\n- Unstable coefficient estimates (large standard errors)\n- Coefficients may have wrong sign\n- Small data changes cause large coefficient changes\n\n**Tree-Based Models:**\n- Less affected but redundancy splits importance\n- May slightly reduce feature importance interpretability\n- Generally robust to multicollinearity\n\n### Recommended Actions for High VIF Features\n1. **Remove one from correlated pairs**: Keep the more interpretable one\n2. **Create composite feature**: Average or PCA combine related features\n3. **Use regularization**: L1 (Lasso) or L2 (Ridge) penalizes correlated features\n4. **Accept for tree models**: If using Random Forest/XGBoost, high VIF is less critical"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Variance Inflation Factor\n",
    "try:\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \n",
    "    # Select features for VIF (exclude target, timestamps, etc.)\n",
    "    vif_features = [col for col in numeric_cols \n",
    "                   if col not in [presence_col, 'month', 'year']][:20]  # Limit for performance\n",
    "    \n",
    "    vif_data = df[vif_features].dropna()\n",
    "    \n",
    "    if len(vif_data) > 0:\n",
    "        vif_results = []\n",
    "        \n",
    "        for i, col in enumerate(vif_features):\n",
    "            try:\n",
    "                vif = variance_inflation_factor(vif_data.values, i)\n",
    "                vif_results.append({'feature': col, 'VIF': vif})\n",
    "            except:\n",
    "                vif_results.append({'feature': col, 'VIF': np.nan})\n",
    "        \n",
    "        vif_df = pd.DataFrame(vif_results).sort_values('VIF', ascending=False)\n",
    "        \n",
    "        print('\\nVariance Inflation Factors:')\n",
    "        print(vif_df)\n",
    "        \n",
    "        # Plot VIF\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(range(len(vif_df)), vif_df['VIF'], color='steelblue', alpha=0.7)\n",
    "        plt.axvline(10, color='red', linestyle='--', linewidth=2, label='VIF=10 threshold')\n",
    "        plt.yticks(range(len(vif_df)), vif_df['feature'])\n",
    "        plt.xlabel('VIF', fontsize=12)\n",
    "        plt.ylabel('Feature', fontsize=12)\n",
    "        plt.title('Variance Inflation Factors (VIF > 10 indicates multicollinearity)', fontsize=14, pad=20)\n",
    "        plt.legend()\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figures_dir / 'vif_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print('\\n✓ Saved VIF analysis')\n",
    "        \n",
    "        # Flag high VIF\n",
    "        high_vif = vif_df[vif_df['VIF'] > 10]\n",
    "        if len(high_vif) > 0:\n",
    "            print(f'\\n⚠ WARNING: {len(high_vif)} features with VIF > 10:')\n",
    "            for _, row in high_vif.iterrows():\n",
    "                print(f\"  - {row['feature']}: VIF = {row['VIF']:.2f}\")\n",
    "        else:\n",
    "            print('\\n✓ No features with VIF > 10')\n",
    "    else:\n",
    "        print('⚠ Insufficient data for VIF calculation')\n",
    "        \n",
    "except ImportError:\n",
    "    print('⚠ statsmodels not available for VIF calculation')\n",
    "except Exception as e:\n",
    "    print(f'⚠ Could not calculate VIF: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Target Correlation\n\nThe most important correlations for predictive modeling are between features and the target variable (elk presence/absence). This section ranks features by their correlation with the binary target.\n\n### What This Code Does\n- Calculates point-biserial correlation (appropriate for binary target)\n- Ranks features by absolute correlation with presence/absence\n- Creates a bar chart showing positive (green) and negative (red) correlations\n\n### Understanding Point-Biserial Correlation\n\nFor binary targets (0/1), point-biserial correlation is equivalent to Pearson correlation and ranges from -1 to +1:\n- **Positive (green bars)**: Higher values → higher presence probability\n- **Negative (red bars)**: Higher values → lower presence probability\n\n### Interpreting Target Correlations\n\n**Correlation Strength:**\n- **|r| > 0.3**: Strong predictor (rare for individual features)\n- **|r| 0.1-0.3**: Moderate predictor (typical for useful features)\n- **|r| < 0.1**: Weak predictor (may still contribute in ensemble)\n\n**Direction Examples for Elk:**\n- **NDVI (+)**: Higher vegetation = more presence (forage)\n- **Slope (-)**: Steeper terrain = less presence (energy cost)\n- **Water distance (-)**: Farther from water = less presence\n- **Road distance (+)**: Farther from roads = more presence (avoidance)\n\n### What to Look For\n- **Top features**: Best candidates for parsimonious models\n- **Sign direction**: Verify ecological intuition\n- **Unexpected strong correlations**: May indicate data issues or discoveries\n- **Weak correlations for expected features**: May indicate data quality issues\n\n### Using Target Correlations for Feature Selection\n1. **Start with top 10**: Build baseline model with strongest predictors\n2. **Add incrementally**: Add features and check for improved performance\n3. **Remove redundant**: If two features have similar target correlation AND high mutual correlation, keep one\n4. **Consider interactions**: Low-correlation features may be powerful in combinations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation with target variable\n",
    "if presence_col:\n",
    "    target_corr = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col != presence_col:\n",
    "            # Use point-biserial correlation for binary target\n",
    "            data = df[[col, presence_col]].dropna()\n",
    "            \n",
    "            if len(data) > 0:\n",
    "                try:\n",
    "                    corr, p_value = pointbiserialr(data[presence_col], data[col])\n",
    "                    target_corr.append({\n",
    "                        'feature': col,\n",
    "                        'correlation': corr,\n",
    "                        'p_value': p_value,\n",
    "                        'abs_correlation': abs(corr)\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    if len(target_corr) > 0:\n",
    "        target_corr_df = pd.DataFrame(target_corr).sort_values('abs_correlation', ascending=False)\n",
    "        \n",
    "        print('\\nFeature correlation with target:')\n",
    "        print(target_corr_df.head(20))\n",
    "        \n",
    "        # Plot top correlations\n",
    "        top_n = min(15, len(target_corr_df))\n",
    "        top_corr = target_corr_df.head(top_n)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        colors = ['green' if x > 0 else 'red' for x in top_corr['correlation']]\n",
    "        plt.barh(range(len(top_corr)), top_corr['correlation'], color=colors, alpha=0.7)\n",
    "        plt.yticks(range(len(top_corr)), top_corr['feature'])\n",
    "        plt.xlabel('Point-Biserial Correlation with Target', fontsize=12)\n",
    "        plt.ylabel('Feature', fontsize=12)\n",
    "        plt.title(f'Top {top_n} Features by Target Correlation', fontsize=14, pad=20)\n",
    "        plt.axvline(0, color='black', linewidth=1)\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figures_dir / 'target_correlations.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print('\\n✓ Saved target correlation plot')\n",
    "        \n",
    "        # Identify top predictive features\n",
    "        print(f'\\nTop 10 most correlated features with target:')\n",
    "        for _, row in target_corr_df.head(10).iterrows():\n",
    "            print(f\"  - {row['feature']}: r = {row['correlation']:.3f}, p = {row['p_value']:.2e}\")\n",
    "    else:\n",
    "        print('⚠ Could not calculate target correlations')\n",
    "else:\n",
    "    print('⚠ Cannot calculate target correlation without presence column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Non-linear Relationships\n\nPearson correlation only measures linear relationships. Important predictors may have non-linear relationships with presence probability (e.g., optimal elevation range). This section visualizes potential non-linear patterns.\n\n### What This Code Does\n- Creates scatter plots of top features vs presence (with jitter)\n- Adds binned mean lines showing how presence rate varies across feature range\n- Reveals non-linear patterns like optimal ranges or thresholds\n\n### Interpreting Non-linear Pattern Plots\n\n**Linear Pattern (diagonal line):**\n- Presence rate increases/decreases monotonically\n- Linear models appropriate\n- Pearson correlation captures relationship well\n\n**Quadratic/Dome Pattern:**\n- Presence peaks at intermediate values\n- Optimal range exists (e.g., ideal elevation band)\n- Add squared term or use tree-based model\n\n**Threshold Pattern:**\n- Presence rate jumps at specific value\n- Hard boundary effect (e.g., snow depth limit)\n- Consider binning or threshold features\n\n**Flat Pattern with Endpoints:**\n- Feature matters only at extreme values\n- May need transformation or interaction\n\n### What to Look For\n- **NDVI**: Should show positive relationship (more green = more presence)\n- **Elevation**: May show dome shape (optimal range for each season)\n- **Snow depth**: Likely threshold effect (too deep limits movement)\n- **Distance features**: Usually monotonic decline with distance\n\n### Implications for Feature Engineering\n1. **Polynomial terms**: Add feature² for dome-shaped relationships\n2. **Binned features**: Convert continuous to categorical for threshold effects\n3. **Log transform**: For features with diminishing returns\n4. **Tree-based models**: Automatically capture non-linearity\n\n### Elk Ecology Context\nNon-linear relationships reflect biological reality:\n- **Elevation**: Elk prefer 7,000-9,000 ft (trade-off forage vs climate)\n- **Slope**: Slightly prefer gentle slopes, but use steep for escape\n- **Snow**: Tolerable up to ~20 inches, then strongly avoid\n- **Temperature**: Prefer moderate temperatures, avoid extremes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine non-linear relationships with target\n",
    "if presence_col and len(target_corr) > 0:\n",
    "    top_features = target_corr_df.head(6)['feature'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feat in enumerate(top_features):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Sample data\n",
    "        plot_df = df[[feat, presence_col]].dropna()\n",
    "        if len(plot_df) > 5000:\n",
    "            plot_df = plot_df.sample(n=5000, random_state=42)\n",
    "        \n",
    "        # Scatter plot with jitter on y-axis\n",
    "        jitter = np.random.normal(0, 0.02, len(plot_df))\n",
    "        ax.scatter(plot_df[feat], plot_df[presence_col] + jitter, alpha=0.1, s=5)\n",
    "        \n",
    "        # Add LOESS smoothing\n",
    "        try:\n",
    "            from scipy.interpolate import UnivariateSpline\n",
    "            \n",
    "            # Sort by feature value\n",
    "            sorted_df = plot_df.sort_values(feat)\n",
    "            \n",
    "            # Bin and calculate mean presence rate\n",
    "            n_bins = 20\n",
    "            bins = pd.qcut(sorted_df[feat], n_bins, duplicates='drop')\n",
    "            binned = sorted_df.groupby(bins)[presence_col].mean()\n",
    "            bin_centers = sorted_df.groupby(bins)[feat].mean()\n",
    "            \n",
    "            ax.plot(bin_centers, binned, 'r-', linewidth=3, label='Binned mean')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        ax.set_xlabel(feat, fontsize=10)\n",
    "        ax.set_ylabel('Presence', fontsize=10)\n",
    "        ax.set_title(f'{feat}', fontsize=11)\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Non-linear Relationships with Target', fontsize=16, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'nonlinear_relationships.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print('✓ Saved non-linear relationship plots')\n",
    "else:\n",
    "    print('⚠ Cannot analyze non-linear relationships')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Correlation Summary\n\nThis section compiles all correlation analysis findings into a summary report with actionable recommendations for feature selection and modeling.\n\n### What This Code Does\n- Summarizes high correlation pairs and their implications\n- Lists top predictive features by target correlation\n- Saves detailed correlation reports to CSV files\n\n### Using the Summary Reports\n\n**correlation_analysis.csv:**\n- Lists all feature pairs with |r| > 0.8\n- Use to decide which features to drop or combine\n- Review before final feature selection\n\n**target_correlations.csv:**\n- Ranks all features by predictive power\n- Use for feature selection prioritization\n- Compare before/after data pipeline changes\n\n### Next Steps After Correlation Analysis\n1. **Remove redundant features**: One from each highly correlated pair\n2. **Address high VIF**: Apply recommended transformations\n3. **Verify ecological sense**: Check that correlations match expectations\n4. **Plan feature engineering**: Create polynomial/interaction terms as needed\n5. **Proceed to Notebook 10**: Validate heuristic scores"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive correlation summary\n",
    "summary = {\n",
    "    'high_correlations': len(high_corr_pairs) if len(high_corr_pairs) > 0 else 0,\n",
    "    'features_analyzed': len(numeric_cols)\n",
    "}\n",
    "\n",
    "if presence_col and len(target_corr) > 0:\n",
    "    summary['top_predictive_features'] = target_corr_df.head(10)['feature'].tolist()\n",
    "    summary['strongest_correlation'] = target_corr_df.iloc[0]['correlation']\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('CORRELATION ANALYSIS SUMMARY')\n",
    "print('='*70)\n",
    "print(f\"\\nFeatures analyzed: {summary['features_analyzed']}\")\n",
    "print(f\"High correlation pairs (|r|>0.8): {summary['high_correlations']}\")\n",
    "\n",
    "if 'top_predictive_features' in summary:\n",
    "    print(f\"\\nTop 10 predictive features:\")\n",
    "    for i, feat in enumerate(summary['top_predictive_features'], 1):\n",
    "        corr_val = target_corr_df[target_corr_df['feature']==feat].iloc[0]['correlation']\n",
    "        print(f\"  {i:2d}. {feat}: r = {corr_val:.3f}\")\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "\n",
    "# Save correlation analysis report\n",
    "if len(high_corr_pairs) > 0:\n",
    "    high_corr_df.to_csv(reports_dir / 'correlation_analysis.csv', index=False)\n",
    "    print(f'\\n✓ Saved correlation analysis to {reports_dir / \"correlation_analysis.csv\"}')\n",
    "\n",
    "if presence_col and len(target_corr) > 0:\n",
    "    target_corr_df.to_csv(reports_dir / 'target_correlations.csv', index=False)\n",
    "    print(f'✓ Saved target correlations to {reports_dir / \"target_correlations.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook analyzed feature correlations:\n",
    "\n",
    "1. **Correlation Matrix**: Identified highly correlated feature pairs\n",
    "2. **Key Pairs**: Visualized strongest correlations\n",
    "3. **Seasonal Correlations**: Examined how relationships change by season\n",
    "4. **Multicollinearity (VIF)**: Flagged features with VIF > 10\n",
    "5. **Target Correlation**: Ranked features by predictive power\n",
    "6. **Non-linear Relationships**: Identified features needing polynomial terms\n",
    "\n",
    "**Key Findings**:\n",
    "- Review `data/reports/correlation_analysis.csv` for redundant features\n",
    "- Review `data/reports/target_correlations.csv` for feature selection\n",
    "- Consider dropping one feature from highly correlated pairs\n",
    "- Use regularization to handle multicollinearity\n",
    "\n",
    "**Recommendations**:\n",
    "- Features with VIF > 10 should be dropped or combined\n",
    "- Top correlated features are good candidates for modeling\n",
    "- Non-linear relationships may benefit from feature engineering\n",
    "\n",
    "**Next Steps**:\n",
    "- Proceed to Notebook 10 for heuristic validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}