{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 6: Data Quality Check\n",
        "\n",
        "## Purpose\n",
        "Identify data quality issues, missing values, outliers, and data integrity problems.\n",
        "\n",
        "## Key Questions\n",
        "- What is the overall completeness of the dataset?\n",
        "- Are there systematic patterns in missing data?\n",
        "- Is NDVI retrieval meeting expectations (>80% success rate)?\n",
        "- Are there outliers or impossible values?\n",
        "- Are GPS coordinates valid?\n",
        "- Is temporal coverage (month/year/day_of_year) complete?\n",
        "- Are rut features (rut_phase, days_since_rut_start) correctly populated during rut season (September-October)?\n",
        "\n",
        "## Key Observations to Look For\n",
        "- **Missing Data**: Should be <20% for most features\n",
        "- **NDVI Range**: Must be between -1.0 and 1.0\n",
        "- **Geographic Bounds**: Within Wyoming (41-45°N, 104-111°W)\n",
        "- **Temporal Coverage**: Multiple years with consistent coverage\n",
        "- **Rut Features**: rut_phase should have valid categories (pre_rut, peak_rut, post_rut, none) and days_since_rut_start should be populated during September-October"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Determine project root and output directories\n",
        "# Try multiple possible paths to find project root\n",
        "possible_roots = [\n",
        "    Path('.'),  # If running from project root\n",
        "    Path('..'),  # If running from notebooks directory\n",
        "    Path('../..'),  # If running from subdirectory\n",
        "]\n",
        "\n",
        "data_root = None\n",
        "for root in possible_roots:\n",
        "    if (root / 'data' / 'features').exists():\n",
        "        data_root = root / 'data'\n",
        "        break\n",
        "\n",
        "if data_root is None:\n",
        "    # Fallback: assume we're in notebooks directory\n",
        "    data_root = Path('../data')\n",
        "\n",
        "# Create output directories relative to project root\n",
        "figures_dir = data_root / 'figures'\n",
        "reports_dir = data_root / 'reports'\n",
        "figures_dir.mkdir(parents=True, exist_ok=True)\n",
        "reports_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f'✓ Setup complete')\n",
        "print(f'  Output directory: {data_root.absolute()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Overview\n",
        "\n",
        "This section loads the complete feature dataset and provides an initial assessment of its structure. Understanding the dataset's shape, column types, and basic statistics establishes a foundation for all subsequent quality checks.\n",
        "\n",
        "### What This Code Does\n",
        "- Loads the combined feature file containing all elk presence/absence observations with environmental context\n",
        "- Displays column names, data types, and memory usage\n",
        "- Shows sample rows to verify data loaded correctly\n",
        "\n",
        "### What to Look For\n",
        "- **Row count**: Should match expected number of observations across all datasets\n",
        "- **Column count**: Should include all expected features (environmental, temporal, spatial, rut features)\n",
        "- **Memory usage**: Large datasets (>500MB) may require chunked processing\n",
        "- **Data types**: Numeric features should be float64/int64, categorical as object\n",
        "- **Rut features**: Check for `rut_phase` and `days_since_rut_start` columns if dataset includes rut behavior features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "from pathlib import Path\n",
        "\n",
        "# Try multiple possible paths\n",
        "possible_paths = [\n",
        "    Path('data/features/complete_context.csv'),  # From project root\n",
        "    Path('../data/features/complete_context.csv'),  # From notebooks directory\n",
        "    Path('../../data/features/complete_context.csv'),  # From subdirectory\n",
        "]\n",
        "\n",
        "data_path = None\n",
        "for path in possible_paths:\n",
        "    if path.exists():\n",
        "        data_path = path\n",
        "        break\n",
        "\n",
        "if data_path is None:\n",
        "    raise FileNotFoundError(\n",
        "        f'Data file not found. Tried: {[str(p) for p in possible_paths]}\\n'\n",
        "        f'Please run: python scripts/combine_feature_files.py\\n'\n",
        "        f'Or ensure you are running the notebook from the project root directory.'\n",
        "    )\n",
        "\n",
        "print(f'Loading data from: {data_path}')\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "print(f'Dataset shape: {df.shape}')\n",
        "print(f'\\nNumber of observations: {df.shape[0]:,}')\n",
        "print(f'Number of features: {df.shape[1]}')\n",
        "print(f'\\nColumn names:')\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f'  {i:2d}. {col}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display data types and memory usage\n",
        "print('Data types:')\n",
        "print(df.dtypes)\n",
        "print(f'\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show first and last rows\n",
        "print('First 10 rows:')\n",
        "display(df.head(10))\n",
        "\n",
        "print('\\nLast 10 rows:')\n",
        "display(df.tail(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect key columns dynamically\n",
        "month_col = None\n",
        "year_col = None\n",
        "day_of_year_cos_col = None\n",
        "day_of_year_sin_col = None\n",
        "lat_col = None\n",
        "lon_col = None\n",
        "presence_col = None\n",
        "\n",
        "# Look for temporal columns\n",
        "for col in df.columns:\n",
        "    if col.lower() == 'month':\n",
        "        month_col = col\n",
        "    elif col.lower() == 'year':\n",
        "        year_col = col\n",
        "    elif col.lower() == 'day_of_year_cos':\n",
        "        day_of_year_cos_col = col\n",
        "    elif col.lower() == 'day_of_year_sin':\n",
        "        day_of_year_sin_col = col\n",
        "\n",
        "# Look for lat/lon\n",
        "for col in df.columns:\n",
        "    if 'lat' in col.lower() and 'lon' not in col.lower():\n",
        "        lat_col = col\n",
        "    if 'lon' in col.lower() and 'lat' not in col.lower():\n",
        "        lon_col = col\n",
        "\n",
        "# Look for presence/target\n",
        "for col in df.columns:\n",
        "    if col.lower() in ['elk_present', 'target', 'label', 'is_presence']:\n",
        "        presence_col = col\n",
        "        break\n",
        "\n",
        "print(f'Detected columns:')\n",
        "print(f'  Month: {month_col}')\n",
        "print(f'  Year: {year_col}')\n",
        "print(f'  Day of Year (cos): {day_of_year_cos_col}')\n",
        "print(f'  Day of Year (sin): {day_of_year_sin_col}')\n",
        "print(f'  Latitude: {lat_col}')\n",
        "print(f'  Longitude: {lon_col}')\n",
        "print(f'  Presence: {presence_col}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show temporal coverage using month and year columns\n",
        "if month_col and year_col:\n",
        "    # Get year range\n",
        "    year_min = int(df[year_col].dropna().min())\n",
        "    year_max = int(df[year_col].dropna().max())\n",
        "    year_range = year_max - year_min + 1\n",
        "    unique_years = sorted(df[year_col].dropna().unique())\n",
        "    \n",
        "    print(f'\\nTemporal coverage:')\n",
        "    print(f'  Years covered: {year_min} to {year_max}')\n",
        "    print(f'  Year range: {year_range} years')\n",
        "    print(f'  Unique years: {len(unique_years)}')\n",
        "    if len(unique_years) <= 10:\n",
        "        print(f'  Year list: {unique_years}')\n",
        "    \n",
        "    # Check month coverage\n",
        "    unique_months = sorted(df[month_col].dropna().unique())\n",
        "    print(f'\\nMonth coverage:')\n",
        "    print(f'  Months present: {unique_months}')\n",
        "    print(f'  Unique months: {len(unique_months)}')\n",
        "    \n",
        "    # Check day_of_year encoding\n",
        "    if day_of_year_cos_col and day_of_year_sin_col:\n",
        "        cos_coverage = df[day_of_year_cos_col].notna().sum()\n",
        "        sin_coverage = df[day_of_year_sin_col].notna().sum()\n",
        "        print(f'\\nDay of year encoding:')\n",
        "        print(f'  day_of_year_cos coverage: {cos_coverage:,} observations ({cos_coverage/len(df)*100:.1f}%)')\n",
        "        print(f'  day_of_year_sin coverage: {sin_coverage:,} observations ({sin_coverage/len(df)*100:.1f}%)')\n",
        "else:\n",
        "    print('\\n⚠ Temporal columns (month/year) not detected')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Geographic bounds\n",
        "if lat_col and lon_col:\n",
        "    print(f'\\nGeographic extent:')\n",
        "    print(f'  Latitude: {df[lat_col].min():.4f}° to {df[lat_col].max():.4f}°')\n",
        "    print(f'  Longitude: {df[lon_col].min():.4f}° to {df[lon_col].max():.4f}°')\n",
        "    print(f'  Unique locations: {df[[lat_col, lon_col]].drop_duplicates().shape[0]:,}')\n",
        "else:\n",
        "    print('\\n⚠ No geographic coordinates detected')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check target variable\n",
        "if presence_col:\n",
        "    print(f'\\nTarget variable ({presence_col}):')\n",
        "    print(df[presence_col].value_counts())\n",
        "    print(f'\\nClass distribution:')\n",
        "    print(df[presence_col].value_counts(normalize=True) * 100)\n",
        "else:\n",
        "    print('\\n⚠ No presence/target column detected')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Observations: Load and Overview\n",
        "\n",
        "After running the cells above, verify the following:\n",
        "\n",
        "- **Total observations**: Should be 400,000+ rows combining all four study areas (Northern Bighorn, Southern Bighorn, National Refuge, Southern GYE)\n",
        "- **Year range**: Should span 2006-2024, covering multiple elk generations and climate cycles\n",
        "- **Month coverage**: All 12 months should be represented; gaps indicate seasonal data collection bias\n",
        "- **Geographic extent**: Latitude 41-45°N, Longitude 104-111°W covers Wyoming elk habitat\n",
        "- **Target distribution**: Ideally close to 50/50 presence/absence for balanced training; >70/30 imbalance requires special handling\n",
        "- **Day of year encoding**: Low coverage in `day_of_year_cos/sin` indicates many records lack precise dates (only month/year known)\n",
        "\n",
        "### Elk Ecology Context\n",
        "The temporal range is critical: elk populations and habitat use patterns have shifted significantly since wolf reintroduction (1995-96) and with recent climate variability. Multi-year data captures both seasonal migration and long-term adaptation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Missing Data Analysis\n",
        "\n",
        "Missing data can significantly impact model performance and may indicate systematic data collection issues. This section identifies which features have missing values and whether the missingness follows patterns (e.g., seasonal gaps in satellite data).\n",
        "\n",
        "### What This Code Does\n",
        "- Calculates missing value counts and percentages for each column\n",
        "- Creates a heatmap visualizing missing data patterns across observations\n",
        "- Analyzes whether missing data correlates with time (month/year)\n",
        "\n",
        "### What to Look For\n",
        "- **Acceptable threshold**: <20% missing for core features (elevation, NDVI, temperature)\n",
        "- **Seasonal patterns**: Winter months typically have more missing NDVI due to cloud cover and snow\n",
        "- **Systematic gaps**: If entire features are mostly missing, they may be placeholder values\n",
        "- **Correlated missingness**: Multiple features missing together suggests data source issues\n",
        "\n",
        "### Elk Ecology Context\n",
        "Some missing data is expected and even informative: NDVI retrieval fails during cloudy/snowy conditions, which often coincides with elk winter range usage at lower elevations. Snow depth data may be missing in summer when SNOTEL stations report zero. This is \"informative missingness\" rather than random gaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate missing value statistics\n",
        "missing_stats = pd.DataFrame({\n",
        "    'column': df.columns,\n",
        "    'missing_count': df.isnull().sum().values,\n",
        "    'missing_pct': (df.isnull().sum() / len(df) * 100).values,\n",
        "    'dtype': df.dtypes.values\n",
        "})\n",
        "\n",
        "missing_stats = missing_stats.sort_values('missing_pct', ascending=False)\n",
        "\n",
        "print('Missing data summary:')\n",
        "print(missing_stats[missing_stats['missing_count'] > 0])\n",
        "\n",
        "# Save to file\n",
        "missing_stats.to_csv(reports_dir / 'missing_data_summary.csv', index=False)\n",
        "print(f'\\n✓ Saved missing data summary to {reports_dir / \"missing_data_summary.csv\"}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flag problematic columns (>20% missing)\n",
        "problematic = missing_stats[missing_stats['missing_pct'] > 20]\n",
        "\n",
        "if len(problematic) > 0:\n",
        "    print(f'\\n⚠ WARNING: {len(problematic)} columns have >20% missing data:')\n",
        "    for _, row in problematic.iterrows():\n",
        "        print(f\"  - {row['column']}: {row['missing_pct']:.1f}% missing\")\n",
        "else:\n",
        "    print('\\n✓ No columns have >20% missing data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create missing data heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Sample if dataset is too large\n",
        "sample_size = min(1000, len(df))\n",
        "df_sample = df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(\n",
        "    df_sample.isnull(),\n",
        "    cbar=True,\n",
        "    yticklabels=False,\n",
        "    cmap='viridis'\n",
        ")\n",
        "plt.title(f'Missing Data Heatmap (sample of {sample_size} rows)\\nYellow = Missing, Purple = Present', \n",
        "          fontsize=14, pad=20)\n",
        "plt.xlabel('Features', fontsize=12)\n",
        "plt.ylabel('Observations', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(figures_dir / 'missing_data_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Saved missing data heatmap')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze missing patterns by month\n",
        "if month_col:\n",
        "    # Calculate missing rate by month\n",
        "    monthly_missing = df.groupby(month_col).apply(\n",
        "        lambda x: (x.isnull().sum() / len(x) * 100).mean()\n",
        "    )\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    monthly_missing_sorted = monthly_missing.sort_index()\n",
        "    plt.bar(monthly_missing_sorted.index, monthly_missing_sorted.values, color='coral')\n",
        "    plt.xlabel('Month', fontsize=12)\n",
        "    plt.ylabel('Average Missing Data (%)', fontsize=12)\n",
        "    plt.title('Missing Data Rate by Month', fontsize=14, pad=20)\n",
        "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "                               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'missing_data_by_month.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('\\nMissing data by month:')\n",
        "    print(monthly_missing_sorted)\n",
        "else:\n",
        "    print('\\n⚠ Cannot analyze temporal patterns without month column')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Observations: Missing Data\n",
        "\n",
        "Review the missing data summary and heatmap above:\n",
        "\n",
        "**Expected Missing Patterns:**\n",
        "- **point_index**: Often 100% missing - this is a metadata field, not a feature\n",
        "- **day_of_year_cos/sin**: High missingness expected if many records only have month/year dates\n",
        "- **snow_station_distance_km**: Missing when no SNOTEL station is within range (~13% typical)\n",
        "- **wolf/bear data quality**: Missing for some study areas without predator monitoring\n",
        "\n",
        "**Concerning Patterns to Flag:**\n",
        "- **>20% missing** in core features (elevation, NDVI, temperature) indicates data pipeline issues\n",
        "- **Constant missing rate** across all months suggests data source problems, not seasonal effects\n",
        "- **Clustering in heatmap** (vertical stripes) indicates batches of problematic records\n",
        "\n",
        "**Interpretation of Monthly Missing Pattern:**\n",
        "- Higher missing rates in winter months (Dec-Feb) are normal for satellite-derived features\n",
        "- If summer months show high missing rates, investigate NDVI/weather data retrieval\n",
        "- Uniform missing across months suggests systematic issues, not seasonal effects\n",
        "\n",
        "**Action Items:**\n",
        "- Features with >50% missing should be dropped or imputed with caution\n",
        "- Consider whether missingness itself is predictive (create \"is_missing\" indicator features)\n",
        "- For modeling, use algorithms that handle missing values (XGBoost, LightGBM) or impute carefully"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. NDVI Quality Checks\n",
        "\n",
        "NDVI (Normalized Difference Vegetation Index) is a critical feature for predicting elk habitat selection, as it directly measures vegetation greenness and forage quality. This section validates that NDVI values are within expected ranges and that retrieval from Google Earth Engine is meeting quality thresholds.\n",
        "\n",
        "### What This Code Does\n",
        "- Calculates NDVI retrieval success rate (% of records with valid NDVI)\n",
        "- Validates that all values fall within the theoretical range [-1, 1]\n",
        "- Visualizes NDVI distribution and seasonal patterns\n",
        "- Analyzes retrieval success by month to identify seasonal gaps\n",
        "\n",
        "### What to Look For\n",
        "- **Success rate threshold**: >80% NDVI retrieval is required for reliable modeling\n",
        "- **Valid range**: All values must be between -1 and 1 (NDVI definition)\n",
        "- **Typical elk habitat**: NDVI 0.2-0.7 for Wyoming grasslands/shrublands\n",
        "- **Seasonal pattern**: Peak in June-August (growing season), lowest in winter\n",
        "\n",
        "### Elk Ecology Context\n",
        "Elk are highly selective for forage quality. NDVI serves as a proxy for the \"green wave\" of spring vegetation emergence that elk track through seasonal migration. Higher NDVI indicates more digestible, protein-rich forage. The Instantaneous Rate of Green-up (IRG) derived from NDVI change is particularly predictive of elk movement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if NDVI column exists\n",
        "# Prefer exact match 'ndvi' over other NDVI-related columns (e.g., ndvi_age_days)\n",
        "ndvi_col = None\n",
        "if 'ndvi' in df.columns:\n",
        "    ndvi_col = 'ndvi'\n",
        "else:\n",
        "    # Fallback: look for any NDVI column but exclude 'age' columns\n",
        "    for col in df.columns:\n",
        "        if 'ndvi' in col.lower() and 'age' not in col.lower() and 'integrated' not in col.lower():\n",
        "            ndvi_col = col\n",
        "            break\n",
        "\n",
        "if ndvi_col:\n",
        "    print(f'Found NDVI column: {ndvi_col}')\n",
        "    \n",
        "    # Calculate retrieval success rate\n",
        "    ndvi_success_rate = (1 - df[ndvi_col].isnull().sum() / len(df)) * 100\n",
        "    print(f'\\nNDVI retrieval success rate: {ndvi_success_rate:.2f}%')\n",
        "    \n",
        "    if ndvi_success_rate < 80:\n",
        "        print(f'⚠ WARNING: NDVI success rate below 80% threshold')\n",
        "    else:\n",
        "        print(f'✓ NDVI success rate meets >80% threshold')\n",
        "    \n",
        "    # Check value range\n",
        "    ndvi_values = df[ndvi_col].dropna()\n",
        "    ndvi_min = ndvi_values.min()\n",
        "    ndvi_max = ndvi_values.max()\n",
        "    \n",
        "    print(f'\\nNDVI value range: {ndvi_min:.4f} to {ndvi_max:.4f}')\n",
        "    \n",
        "    # Flag invalid values\n",
        "    invalid_ndvi = ((ndvi_values < -1) | (ndvi_values > 1)).sum()\n",
        "    if invalid_ndvi > 0:\n",
        "        print(f'\\n⚠ CRITICAL: Found {invalid_ndvi} NDVI values outside valid range [-1, 1]')\n",
        "    else:\n",
        "        print(f'✓ All NDVI values within valid range [-1, 1]')\n",
        "else:\n",
        "    print('⚠ No NDVI column found in dataset')\n",
        "    ndvi_success_rate = 100\n",
        "    invalid_ndvi = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot NDVI distribution\n",
        "if ndvi_col:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Histogram\n",
        "    axes[0].hist(df[ndvi_col].dropna(), bins=50, color='green', alpha=0.7, edgecolor='black')\n",
        "    axes[0].axvline(df[ndvi_col].mean(), color='red', linestyle='--', linewidth=2, \n",
        "                    label=f'Mean: {df[ndvi_col].mean():.3f}')\n",
        "    axes[0].axvline(df[ndvi_col].median(), color='blue', linestyle='--', linewidth=2, \n",
        "                    label=f'Median: {df[ndvi_col].median():.3f}')\n",
        "    axes[0].set_xlabel('NDVI Value', fontsize=12)\n",
        "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "    axes[0].set_title('NDVI Distribution', fontsize=14)\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Box plot\n",
        "    axes[1].boxplot(df[ndvi_col].dropna(), vert=True)\n",
        "    axes[1].set_ylabel('NDVI Value', fontsize=12)\n",
        "    axes[1].set_title('NDVI Box Plot', fontsize=14)\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'ndvi_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('✓ Saved NDVI distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monthly NDVI pattern\n",
        "if ndvi_col and month_col:\n",
        "    monthly_ndvi = df.groupby(month_col)[ndvi_col].agg(['mean', 'std', 'count'])\n",
        "    monthly_ndvi_sorted = monthly_ndvi.sort_index()\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(monthly_ndvi_sorted.index, monthly_ndvi_sorted['mean'], marker='o', \n",
        "             linewidth=2, markersize=8, color='green')\n",
        "    plt.fill_between(\n",
        "        monthly_ndvi_sorted.index,\n",
        "        monthly_ndvi_sorted['mean'] - monthly_ndvi_sorted['std'],\n",
        "        monthly_ndvi_sorted['mean'] + monthly_ndvi_sorted['std'],\n",
        "        alpha=0.3,\n",
        "        color='green'\n",
        "    )\n",
        "    plt.xlabel('Month', fontsize=12)\n",
        "    plt.ylabel('Mean NDVI', fontsize=12)\n",
        "    plt.title('NDVI Seasonal Pattern (Mean ± Std)', fontsize=14, pad=20)\n",
        "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "                               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'ndvi_seasonal_pattern.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('\\nMonthly NDVI statistics:')\n",
        "    print(monthly_ndvi_sorted)\n",
        "    print('\\n✓ Saved NDVI seasonal pattern')\n",
        "else:\n",
        "    print('\\n⚠ Cannot analyze NDVI seasonal patterns without NDVI and month columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NDVI success rate by month\n",
        "if ndvi_col and month_col:\n",
        "    monthly_success = df.groupby(month_col).apply(\n",
        "        lambda x: (1 - x[ndvi_col].isnull().sum() / len(x)) * 100\n",
        "    )\n",
        "    monthly_success_sorted = monthly_success.sort_index()\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    colors = ['red' if x < 80 else 'green' for x in monthly_success_sorted.values]\n",
        "    plt.bar(monthly_success_sorted.index, monthly_success_sorted.values, color=colors, alpha=0.7, edgecolor='black')\n",
        "    plt.axhline(80, color='red', linestyle='--', linewidth=2, label='80% threshold')\n",
        "    plt.xlabel('Month', fontsize=12)\n",
        "    plt.ylabel('NDVI Retrieval Success Rate (%)', fontsize=12)\n",
        "    plt.title('NDVI Retrieval Success Rate by Month', fontsize=14, pad=20)\n",
        "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "                               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "    plt.ylim(0, 105)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'ndvi_success_by_month.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('\\nNDVI success rate by month:')\n",
        "    print(monthly_success_sorted)\n",
        "    \n",
        "    low_months = monthly_success_sorted[monthly_success_sorted < 80]\n",
        "    if len(low_months) > 0:\n",
        "        print(f'\\n⚠ WARNING: {len(low_months)} months have <80% NDVI success:')\n",
        "        for month, rate in low_months.items():\n",
        "            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "            month_int = int(month)\n",
        "            if 1 <= month_int <= 12:\n",
        "                print(f'  - {month_names[month_int-1]}: {rate:.1f}%')\n",
        "else:\n",
        "    print('\\n⚠ Cannot analyze NDVI success by month without NDVI and month columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save NDVI quality report\n",
        "if ndvi_col:\n",
        "    report = f'''NDVI Quality Report\n",
        "===================\n",
        "Generated: {pd.Timestamp.now()}\n",
        "\n",
        "Overall Statistics:\n",
        "- Total observations: {len(df):,}\n",
        "- NDVI available: {df[ndvi_col].notna().sum():,}\n",
        "- NDVI missing: {df[ndvi_col].isnull().sum():,}\n",
        "- Success rate: {ndvi_success_rate:.2f}%\n",
        "\n",
        "Value Range:\n",
        "- Minimum: {ndvi_min:.4f}\n",
        "- Maximum: {ndvi_max:.4f}\n",
        "- Mean: {df[ndvi_col].mean():.4f}\n",
        "- Median: {df[ndvi_col].median():.4f}\n",
        "- Std Dev: {df[ndvi_col].std():.4f}\n",
        "\n",
        "Data Quality:\n",
        "- Invalid values (outside [-1, 1]): {invalid_ndvi}\n",
        "- Status: {'PASS' if invalid_ndvi == 0 and ndvi_success_rate >= 80 else 'FAIL'}\n",
        "'''\n",
        "    \n",
        "    with open(reports_dir / 'ndvi_quality.txt', 'w') as f:\n",
        "        f.write(report)\n",
        "    \n",
        "    print(f'\\n✓ Saved NDVI quality report to {reports_dir / \"ndvi_quality.txt\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Observations: NDVI Quality\n",
        "\n",
        "Review the NDVI analysis above and verify:\n",
        "\n",
        "**Success Rate Interpretation:**\n",
        "- **>90%**: Excellent - GEE retrieval pipeline is working well\n",
        "- **80-90%**: Acceptable - some cloud/snow gaps are normal\n",
        "- **<80%**: Concerning - investigate data pipeline or date range issues\n",
        "\n",
        "**Distribution Shape:**\n",
        "- **Bimodal distribution**: May indicate mixing of summer (high NDVI) and winter (low NDVI) observations\n",
        "- **Narrow range (0.3-0.7)**: If NDVI shows near-zero variance, check for placeholder values\n",
        "- **Left skew**: More low-NDVI observations than expected may indicate winter bias in data collection\n",
        "\n",
        "**Seasonal Pattern Expectations:**\n",
        "- **Peak (June-August)**: NDVI should reach 0.5-0.7 during growing season\n",
        "- **Trough (December-February)**: NDVI drops to 0.2-0.4 for dormant vegetation\n",
        "- **Spring green-up (April-May)**: Rapid increase is when elk are most responsive to NDVI\n",
        "- **Flat line**: If seasonal pattern is flat, data may contain placeholder values\n",
        "\n",
        "**Monthly Success Rate:**\n",
        "- Winter months (Dec-Feb) showing <70% success is acceptable (cloud cover)\n",
        "- Summer months should have >90% success\n",
        "- If all months show same success rate, verify actual satellite data is being retrieved\n",
        "\n",
        "**Red Flags:**\n",
        "- All NDVI values identical (constant/placeholder value)\n",
        "- NDVI values outside [-1, 1] range (computation error)\n",
        "- No seasonal variation (data not from real satellite observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Outlier Detection\n",
        "\n",
        "Outliers can represent data errors, extreme but valid observations, or edge cases that may disproportionately influence model training. This section identifies statistical outliers using z-scores and visualizes their distribution.\n",
        "\n",
        "### What This Code Does\n",
        "- Calculates z-scores for all numeric features\n",
        "- Flags observations with |z-score| > 3 (beyond 3 standard deviations)\n",
        "- Creates box plots to visualize outlier distribution\n",
        "- Saves outlier records for manual review\n",
        "\n",
        "### What to Look For\n",
        "- **Outlier count by feature**: Some features naturally have more outliers (snow depth, distance measures)\n",
        "- **Extreme z-scores (>10)**: Likely data errors rather than natural variation\n",
        "- **Clustered outliers**: Multiple outliers in same record may indicate bad GPS fix or sensor malfunction\n",
        "\n",
        "### Interpreting Box Plots\n",
        "- **Whiskers**: Extend to 1.5x IQR (interquartile range)\n",
        "- **Points beyond whiskers**: Statistical outliers\n",
        "- **Box width**: Narrower boxes indicate less variation\n",
        "- **Median line**: Should be roughly centered for symmetric distributions\n",
        "\n",
        "### Elk Ecology Context\n",
        "Extreme values may be ecologically meaningful: elk occasionally use very high elevations (>10,000 ft) in summer or travel far from water during migration. Context matters - an \"outlier\" elevation reading during September may represent normal alpine habitat use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f'Found {len(numeric_cols)} numeric columns')\n",
        "\n",
        "# Calculate z-scores and identify outliers\n",
        "outlier_counts = {}\n",
        "outlier_records = []\n",
        "\n",
        "for col in numeric_cols:\n",
        "    values = df[col].dropna()\n",
        "    if len(values) > 0 and values.std() > 0:\n",
        "        z_scores = np.abs(stats.zscore(values))\n",
        "        outliers = z_scores > 3\n",
        "        outlier_count = outliers.sum()\n",
        "        outlier_counts[col] = outlier_count\n",
        "        \n",
        "        # Store outlier records\n",
        "        if outlier_count > 0:\n",
        "            outlier_idx = values[outliers].index\n",
        "            for idx in outlier_idx:\n",
        "                outlier_records.append({\n",
        "                    'index': idx,\n",
        "                    'column': col,\n",
        "                    'value': df.loc[idx, col],\n",
        "                    'z_score': z_scores[values.index.get_loc(idx)]\n",
        "                })\n",
        "\n",
        "# Sort by outlier count\n",
        "outlier_summary = pd.DataFrame([\n",
        "    {'column': col, 'outlier_count': count, 'outlier_pct': count/len(df)*100}\n",
        "    for col, count in outlier_counts.items()\n",
        "]).sort_values('outlier_count', ascending=False)\n",
        "\n",
        "print('\\nOutlier counts (|z-score| > 3):')\n",
        "print(outlier_summary[outlier_summary['outlier_count'] > 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create box plots for features with most outliers\n",
        "top_outlier_cols = outlier_summary.head(12)['column'].tolist()\n",
        "\n",
        "if len(top_outlier_cols) > 0:\n",
        "    n_cols = 3\n",
        "    n_rows = (len(top_outlier_cols) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "    \n",
        "    for idx, col in enumerate(top_outlier_cols):\n",
        "        ax = axes[idx]\n",
        "        data = df[col].dropna()\n",
        "        ax.boxplot(data, vert=True)\n",
        "        ax.set_ylabel(col, fontsize=10)\n",
        "        ax.set_title(f'{col}\\n({outlier_counts[col]} outliers)', fontsize=11)\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(top_outlier_cols), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle('Box Plots for Features with Most Outliers', fontsize=16, y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'outlier_boxplots.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('✓ Saved outlier box plots')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save outlier records for manual review\n",
        "if len(outlier_records) > 0:\n",
        "    outlier_df = pd.DataFrame(outlier_records)\n",
        "    outlier_df = outlier_df.sort_values('z_score', ascending=False)\n",
        "    outlier_df.to_csv(reports_dir / 'outliers.csv', index=False)\n",
        "    \n",
        "    print(f'\\n✓ Saved {len(outlier_records)} outlier records to {reports_dir / \"outliers.csv\"}')\n",
        "    print(f'\\nTop 10 most extreme outliers:')\n",
        "    print(outlier_df.head(10))\n",
        "else:\n",
        "    print('\\n✓ No outliers detected (|z-score| > 3)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Observations: Outliers\n",
        "\n",
        "Review the outlier analysis and box plots above:\n",
        "\n",
        "**Features with Expected High Outlier Counts:**\n",
        "- **snow_depth_inches / snow_water_equiv_inches**: Right-skewed distributions with occasional deep snow events\n",
        "- **security_habitat_percent**: Most areas have low security habitat; a few locations have very high values\n",
        "- **slope_degrees**: Most terrain is gentle; steep slopes are outliers but ecologically important\n",
        "- **water_distance_miles**: Most observations are near water; distant locations are rare but valid\n",
        "\n",
        "**Features Where Outliers Indicate Problems:**\n",
        "- **elevation**: Values <1,000 ft or >14,000 ft in Wyoming are likely GPS errors\n",
        "- **temperature_f**: Values <-60°F or >120°F are sensor errors\n",
        "- **NDVI**: Values outside [-1, 1] are computation errors (should not occur)\n",
        "- **latitude/longitude**: Values outside Wyoming bounds indicate bad coordinates\n",
        "\n",
        "**Interpreting the Top Outliers Report:**\n",
        "- **z-score 15-20**: Very extreme, likely data error - verify in source data\n",
        "- **z-score 5-10**: Unusual but potentially valid - check ecological context\n",
        "- **z-score 3-5**: Moderate outliers, typically valid extreme observations\n",
        "\n",
        "**Action Items:**\n",
        "- Review records with z-score > 10 in `data/reports/outliers.csv`\n",
        "- Do NOT automatically remove all outliers - many represent valid edge cases\n",
        "- Consider winsorizing (capping) extreme values rather than removing\n",
        "- For modeling, use robust algorithms or log-transform heavily skewed features\n",
        "\n",
        "**Elk-Specific Outlier Validation:**\n",
        "- High elevation outliers in summer: Likely valid (alpine summer range)\n",
        "- High elevation outliers in winter: Suspicious (elk descend in winter)\n",
        "- Large water distances: May be valid during migration corridors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Geographic Validation\n",
        "\n",
        "GPS coordinates are the foundation of all spatial features (elevation, land cover, distances). This section validates that all observations fall within expected geographic bounds and visualizes spatial coverage.\n",
        "\n",
        "### What This Code Does\n",
        "- Checks that all coordinates fall within Wyoming state boundaries\n",
        "- Creates a scatter plot map showing observation locations\n",
        "- Colors points by presence/absence to reveal spatial patterns\n",
        "- Calculates the number of unique locations\n",
        "\n",
        "### What to Look For\n",
        "- **Out-of-bounds coordinates**: Any points outside Wyoming indicate GPS errors or data issues\n",
        "- **Spatial clustering**: Elk observations should cluster in known habitat areas\n",
        "- **Absence point distribution**: Pseudo-absences should cover similar geographic extent as presences\n",
        "- **Coverage gaps**: Large areas without observations may bias the model\n",
        "\n",
        "### Wyoming Geographic Bounds\n",
        "- **Latitude**: 41.0°N to 45.0°N\n",
        "- **Longitude**: -111.0°W to -104.0°W\n",
        "- Slight exceedances (within 0.1°) may be valid for border areas\n",
        "\n",
        "### Elk Ecology Context\n",
        "The four study areas represent distinct elk populations:\n",
        "- **Northern Bighorn**: High-elevation alpine/subalpine habitat\n",
        "- **Southern Bighorn**: Lower elevation sagebrush-grassland interface\n",
        "- **National Elk Refuge**: Winter concentration area near Jackson\n",
        "- **Southern Greater Yellowstone**: Diverse habitat across the GYE southern boundary\n",
        "\n",
        "Spatial separation between study areas means the model should learn generalizable habitat relationships, not location-specific patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate geographic coordinates\n",
        "if lat_col and lon_col:\n",
        "    # Wyoming bounds (approximate)\n",
        "    WY_LAT_MIN, WY_LAT_MAX = 41.0, 45.0\n",
        "    WY_LON_MIN, WY_LON_MAX = -111.0, -104.0\n",
        "    \n",
        "    # Check bounds\n",
        "    invalid_lat = ((df[lat_col] < WY_LAT_MIN) | (df[lat_col] > WY_LAT_MAX)).sum()\n",
        "    invalid_lon = ((df[lon_col] < WY_LON_MIN) | (df[lon_col] > WY_LON_MAX)).sum()\n",
        "    \n",
        "    print(f'Geographic validation:')\n",
        "    print(f'  Wyoming bounds: {WY_LAT_MIN}°-{WY_LAT_MAX}°N, {WY_LON_MIN}°-{WY_LON_MAX}°W')\n",
        "    print(f'  Latitude range: {df[lat_col].min():.4f}° to {df[lat_col].max():.4f}°')\n",
        "    print(f'  Longitude range: {df[lon_col].min():.4f}° to {df[lon_col].max():.4f}°')\n",
        "    print(f'  Invalid latitudes: {invalid_lat}')\n",
        "    print(f'  Invalid longitudes: {invalid_lon}')\n",
        "    \n",
        "    if invalid_lat > 0 or invalid_lon > 0:\n",
        "        print(f'\\n⚠ WARNING: Found coordinates outside Wyoming bounds')\n",
        "    else:\n",
        "        print(f'\\n✓ All coordinates within Wyoming bounds')\n",
        "else:\n",
        "    print('⚠ Cannot validate geographic coordinates - columns not found')\n",
        "    invalid_lat = 0\n",
        "    invalid_lon = 0\n",
        "    WY_LAT_MIN, WY_LAT_MAX = 41.0, 45.0\n",
        "    WY_LON_MIN, WY_LON_MAX = -111.0, -104.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create GPS coverage map\n",
        "if lat_col and lon_col:\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    \n",
        "    # Sample if too many points\n",
        "    sample_size = min(10000, len(df))\n",
        "    df_sample = df.sample(n=sample_size, random_state=42)\n",
        "    \n",
        "    # Color by presence if available\n",
        "    if presence_col:\n",
        "        colors = df_sample[presence_col].map({1: 'blue', 0: 'red', True: 'blue', False: 'red'})\n",
        "        plt.scatter(\n",
        "            df_sample[lon_col],\n",
        "            df_sample[lat_col],\n",
        "            c=colors,\n",
        "            alpha=0.3,\n",
        "            s=10,\n",
        "            edgecolors='none'\n",
        "        )\n",
        "        # Create legend\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elements = [\n",
        "            Patch(facecolor='blue', alpha=0.5, label='Presence'),\n",
        "            Patch(facecolor='red', alpha=0.5, label='Absence')\n",
        "        ]\n",
        "        plt.legend(handles=legend_elements, loc='upper right')\n",
        "    else:\n",
        "        plt.scatter(\n",
        "            df_sample[lon_col],\n",
        "            df_sample[lat_col],\n",
        "            alpha=0.3,\n",
        "            s=10,\n",
        "            color='blue',\n",
        "            edgecolors='none'\n",
        "        )\n",
        "    \n",
        "    # Add Wyoming bounds\n",
        "    plt.axhline(WY_LAT_MIN, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    plt.axhline(WY_LAT_MAX, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    plt.axvline(WY_LON_MIN, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    plt.axvline(WY_LON_MAX, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    \n",
        "    plt.xlabel('Longitude', fontsize=12)\n",
        "    plt.ylabel('Latitude', fontsize=12)\n",
        "    plt.title(f'GPS Coverage Map (sample of {sample_size} points)', fontsize=14, pad=20)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'gps_coverage_map.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('✓ Saved GPS coverage map')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Observations: Geographic Validation\n",
        "\n",
        "Review the geographic validation and coverage map above:\n",
        "\n",
        "**Coordinate Bounds Check:**\n",
        "- **All coordinates in bounds**: Ideal outcome - data is geographically valid\n",
        "- **Small number out of bounds (<1%)**: May be border areas or minor GPS drift - review individually\n",
        "- **Large number out of bounds (>1%)**: Indicates systematic GPS errors or incorrect coordinate system\n",
        "\n",
        "**Coverage Map Interpretation:**\n",
        "- **Distinct clusters**: Expected - should see 4 clusters corresponding to study areas\n",
        "- **Presence/absence overlap**: Good - pseudo-absences should be spatially interspersed with presences\n",
        "- **Color separation**: If presence (blue) and absence (red) are completely separated spatially, the model may learn location instead of habitat\n",
        "- **Dense vs sparse areas**: Denser observations provide more reliable local predictions\n",
        "\n",
        "**Study Area Identification:**\n",
        "Looking at the map, you should be able to identify:\n",
        "1. **NW cluster**: National Elk Refuge / Jackson area (~43.5°N, -110.5°W)\n",
        "2. **West-central cluster**: Southern GYE (~43-44°N, -110°W)\n",
        "3. **East clusters**: Bighorn Mountains (~44°N, -107°W)\n",
        "\n",
        "**Spatial Bias Concerns:**\n",
        "- If one study area dominates the dataset, model may be biased toward that area's conditions\n",
        "- Ideal: roughly equal representation from each study area\n",
        "- Check: Are absence points generated across all study areas or concentrated in some?\n",
        "\n",
        "**Action Items:**\n",
        "- Remove records with clearly invalid coordinates (outside Wyoming by >1°)\n",
        "- Note spatial coverage limitations in model interpretation\n",
        "- Consider spatial cross-validation (leave-one-study-area-out) for robust evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Temporal Coverage\n",
        "\n",
        "Temporal coverage affects model generalizability across seasons and years. This section analyzes observation distribution across months and years to identify potential temporal biases.\n",
        "\n",
        "### What This Code Does\n",
        "- Counts observations by month to check seasonal coverage\n",
        "- Counts observations by year to check annual coverage\n",
        "- Visualizes temporal distribution with bar charts\n",
        "- Flags months/years with insufficient data\n",
        "\n",
        "### What to Look For\n",
        "- **Monthly coverage**: All 12 months should be represented for seasonal generalization\n",
        "- **Seasonal balance**: No single season should dominate (unless studying seasonal patterns)\n",
        "- **Annual trends**: Coverage should be consistent across years or trends should be understood\n",
        "- **Minimum threshold**: At least 100 observations per month for stable statistics\n",
        "\n",
        "### Elk Ecology Context\n",
        "Elk behavior varies dramatically by season:\n",
        "- **Winter (Dec-Feb)**: Concentrated on winter range, limited movement\n",
        "- **Spring (Mar-May)**: Migration, following green-up\n",
        "- **Summer (Jun-Aug)**: Dispersed on summer range, high elevations\n",
        "- **Fall (Sep-Nov)**: Rut, migration to winter range, hunting season\n",
        "\n",
        "Data collected primarily during one season may not generalize to year-round habitat prediction. GPS collar studies often have better coverage during specific seasons based on study objectives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze temporal coverage\n",
        "if month_col:\n",
        "    # Observations by month\n",
        "    monthly_counts = df.groupby(month_col).size()\n",
        "    monthly_counts_sorted = monthly_counts.sort_index()\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    colors = ['red' if x < 100 else 'green' for x in monthly_counts_sorted.values]\n",
        "    plt.bar(monthly_counts_sorted.index, monthly_counts_sorted.values, color=colors, alpha=0.7, edgecolor='black')\n",
        "    plt.axhline(100, color='red', linestyle='--', linewidth=2, label='100 obs threshold')\n",
        "    plt.xlabel('Month', fontsize=12)\n",
        "    plt.ylabel('Number of Observations', fontsize=12)\n",
        "    plt.title('Observation Count by Month', fontsize=14, pad=20)\n",
        "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "                               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'temporal_coverage.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('\\nObservations by month:')\n",
        "    print(monthly_counts_sorted)\n",
        "    \n",
        "    # Identify gaps\n",
        "    gaps = monthly_counts_sorted[monthly_counts_sorted < 100]\n",
        "    if len(gaps) > 0:\n",
        "        print(f'\\n⚠ WARNING: {len(gaps)} months have <100 observations')\n",
        "    else:\n",
        "        print('\\n✓ All months have >100 observations')\n",
        "else:\n",
        "    print('⚠ Cannot analyze temporal coverage without month column')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Observations by year\n",
        "if year_col:\n",
        "    yearly_counts = df.groupby(year_col).size()\n",
        "    yearly_counts_sorted = yearly_counts.sort_index()\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(yearly_counts_sorted.index, yearly_counts_sorted.values, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "    plt.xlabel('Year', fontsize=12)\n",
        "    plt.ylabel('Number of Observations', fontsize=12)\n",
        "    plt.title('Observation Count by Year', fontsize=14, pad=20)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_dir / 'temporal_coverage_yearly.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print('\\nObservations by year:')\n",
        "    print(yearly_counts_sorted)\n",
        "else:\n",
        "    print('⚠ Cannot analyze yearly coverage without year column')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Observations: Temporal Coverage\n",
        "\n",
        "Review the monthly and yearly bar charts above:\n",
        "\n",
        "**Monthly Distribution Interpretation:**\n",
        "- **Uniform distribution**: Ideal - model can learn all seasonal patterns\n",
        "- **Winter-heavy**: Model may overfit to winter range characteristics\n",
        "- **Summer-heavy**: Model may overfit to summer range characteristics\n",
        "- **Missing months**: Critical gap - model cannot predict for those seasons\n",
        "\n",
        "**Yearly Distribution Interpretation:**\n",
        "- **Consistent across years**: Good temporal generalization expected\n",
        "- **Recent years dominant**: Model reflects current conditions, may miss long-term patterns\n",
        "- **Older years dominant**: Model may not capture recent habitat changes\n",
        "- **Gaps (missing years)**: May miss important climate events or population changes\n",
        "\n",
        "**Expected Patterns:**\n",
        "- Months 1-4 (Jan-Apr): Often higher counts due to winter range concentration studies\n",
        "- Months 5-8 (May-Aug): May be lower if summer range is harder to access\n",
        "- Months 9-12 (Sep-Dec): Hunting season may affect data collection\n",
        "\n",
        "**Data Collection Artifacts:**\n",
        "- **GPS collar fix rate**: May vary by season (battery, terrain, behavior)\n",
        "- **Study duration**: Multi-year studies have more balanced temporal coverage\n",
        "- **Research objectives**: Some studies target specific seasons\n",
        "\n",
        "**Action Items:**\n",
        "- If severe seasonal imbalance, consider stratified sampling for model training\n",
        "- Include month as a feature to help model learn seasonal variation\n",
        "- For prediction, be cautious applying model to underrepresented seasons\n",
        "- Consider separate models for winter vs summer if behavioral patterns differ greatly\n",
        "\n",
        "**Train/Test Split Implications:**\n",
        "- Random split may have different seasonal composition in train vs test\n",
        "- Consider temporal split (train on earlier years, test on recent) for realistic evaluation\n",
        "- Stratify by month to ensure all seasons represented in both train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Value Ranges\n",
        "\n",
        "This section validates that all feature values fall within physically plausible ranges and flags features with suspicious characteristics like zero variance (constant values) or impossible values.\n",
        "\n",
        "### What This Code Does\n",
        "- Calculates summary statistics (min, max, mean, std) for all numeric features\n",
        "- Flags features with near-zero variance (potential placeholder values)\n",
        "- Identifies values outside physically possible ranges\n",
        "- Checks for features that might be incorrectly scaled or encoded\n",
        "\n",
        "### What to Look For\n",
        "- **Zero variance**: std ≈ 0 indicates constant/placeholder values - NOT real data\n",
        "- **Physical bounds**: Temperature, NDVI, percentages should be within valid ranges\n",
        "- **Negative values**: Distance and percentage features should not be negative\n",
        "- **Extreme ranges**: Min-max spread should be plausible for Wyoming conditions\n",
        "\n",
        "### Expected Value Ranges for Key Features\n",
        "\n",
        "| Feature | Expected Range | Red Flag |\n",
        "|---------|---------------|----------|\n",
        "| elevation | 900-14,000 ft | <500 or >15,000 |\n",
        "| temperature_f | -40 to 110°F | <-60 or >120 |\n",
        "| ndvi | -1 to 1 | Outside this range |\n",
        "| slope_degrees | 0-90° | >90 or negative |\n",
        "| canopy_cover_percent | 0-100% | >100 or negative |\n",
        "| snow_depth_inches | 0-200 in | Negative |\n",
        "| distance features | 0+ miles | Negative |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics for all numeric features\n",
        "numeric_summary = df[numeric_cols].describe().T\n",
        "numeric_summary['missing_pct'] = (df[numeric_cols].isnull().sum() / len(df) * 100).values\n",
        "numeric_summary = numeric_summary[['count', 'mean', 'std', 'min', 'max', 'missing_pct']]\n",
        "\n",
        "print('Summary statistics for numeric features:')\n",
        "print(numeric_summary)\n",
        "\n",
        "# Save to file\n",
        "numeric_summary.to_csv(reports_dir / 'feature_ranges.csv')\n",
        "print(f'\\n✓ Saved feature ranges to {reports_dir / \"feature_ranges.csv\"}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flag suspicious values\n",
        "suspicious = []\n",
        "\n",
        "# Check for common issues\n",
        "for col in numeric_cols:\n",
        "    col_lower = col.lower()\n",
        "    \n",
        "    # Temperature checks\n",
        "    if 'temp' in col_lower and 'celsius' not in col_lower:\n",
        "        extreme_hot = (df[col] > 120).sum()\n",
        "        extreme_cold = (df[col] < -60).sum()\n",
        "        if extreme_hot > 0:\n",
        "            suspicious.append(f'{col}: {extreme_hot} values > 120°F')\n",
        "        if extreme_cold > 0:\n",
        "            suspicious.append(f'{col}: {extreme_cold} values < -60°F')\n",
        "    \n",
        "    # Elevation checks\n",
        "    if 'elev' in col_lower or 'altitude' in col_lower:\n",
        "        too_low = (df[col] < 0).sum()\n",
        "        too_high = (df[col] > 15000).sum()\n",
        "        if too_low > 0:\n",
        "            suspicious.append(f'{col}: {too_low} values < 0')\n",
        "        if too_high > 0:\n",
        "            suspicious.append(f'{col}: {too_high} values > 15,000 ft')\n",
        "    \n",
        "    # Check for zero variance\n",
        "    if df[col].std() < 1e-10:\n",
        "        suspicious.append(f'{col}: Near-zero variance (std={df[col].std():.2e})')\n",
        "\n",
        "if len(suspicious) > 0:\n",
        "    print('\\n⚠ Suspicious value ranges detected:')\n",
        "    for issue in suspicious:\n",
        "        print(f'  - {issue}')\n",
        "else:\n",
        "    print('\\n✓ No obviously suspicious value ranges detected')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Observations: Feature Ranges\n",
        "\n",
        "Review the feature summary table and suspicious values report above:\n",
        "\n",
        "**Critical Issue: Zero/Near-Zero Variance Features**\n",
        "Features with std ≈ 0 are placeholder values, not real data. Common culprits:\n",
        "- **temperature_f = 42.0**: Placeholder - should vary by season (-20°F to 90°F)\n",
        "- **cloud_cover_percent = 40.0**: Placeholder - should vary 0-100%\n",
        "- **road_distance_miles = 2.0**: Placeholder - should vary by location\n",
        "- **trail_distance_miles = 1.5**: Placeholder - should vary by location\n",
        "- **precip_last_7_days_inches = 0.5**: Placeholder - should vary by weather\n",
        "- **ndvi_age_days = 8.0**: Placeholder - indicates stale NDVI retrieval\n",
        "\n",
        "**Action Required for Placeholder Values:**\n",
        "These features provide NO information and should be either:\n",
        "1. **Re-populated** by re-running the data pipeline with fixed data sources\n",
        "2. **Removed** from the feature set before modeling\n",
        "3. **Flagged** with a quality indicator for conditional use\n",
        "\n",
        "**Validation Checklist:**\n",
        "- [ ] Elevation ranges match Wyoming topography (900-14,000 ft)\n",
        "- [ ] Temperature shows seasonal variation (-40°F to 110°F)\n",
        "- [ ] NDVI is within [-1, 1] and shows seasonal cycle\n",
        "- [ ] Snow depth is non-negative and shows winter peaks\n",
        "- [ ] All percentages are between 0-100%\n",
        "- [ ] All distances are non-negative\n",
        "- [ ] Lat/Lon are within Wyoming bounds\n",
        "\n",
        "**Priority Issues to Address:**\n",
        "1. Features with constant values (zero variance) - investigate data pipeline\n",
        "2. Features with impossible values - data entry or computation errors\n",
        "3. Features with unexpected ranges - verify units and scaling\n",
        "\n",
        "**Impact on Modeling:**\n",
        "- Constant features add no predictive power and should be removed\n",
        "- Features with many impossible values may need cleaning or removal\n",
        "- Wide ranges may benefit from normalization/standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Overall Data Quality Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate overall data quality score\n",
        "def calculate_quality_score(df):\n",
        "    '''Calculate overall data quality score (0-100)'''\n",
        "    scores = {}\n",
        "    \n",
        "    # 1. Completeness (40 points)\n",
        "    completeness = (1 - df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100\n",
        "    scores['completeness'] = min(completeness / 100 * 40, 40)\n",
        "    \n",
        "    # 2. Validity (30 points) - based on outliers\n",
        "    total_outliers = sum(outlier_counts.values())\n",
        "    outlier_rate = total_outliers / (df.shape[0] * len(numeric_cols)) if len(numeric_cols) > 0 else 0\n",
        "    validity = (1 - outlier_rate) * 100\n",
        "    scores['validity'] = min(validity / 100 * 30, 30)\n",
        "    \n",
        "    # 3. Geographic validity (15 points)\n",
        "    if lat_col and lon_col:\n",
        "        geo_valid_rate = 1 - (invalid_lat + invalid_lon) / (len(df) * 2)\n",
        "        scores['geographic'] = geo_valid_rate * 15\n",
        "    else:\n",
        "        scores['geographic'] = 15\n",
        "    \n",
        "    # 4. NDVI quality (15 points)\n",
        "    if ndvi_col:\n",
        "        ndvi_quality = (ndvi_success_rate / 100) * (1 if invalid_ndvi == 0 else 0.5)\n",
        "        scores['ndvi'] = ndvi_quality * 15\n",
        "    else:\n",
        "        scores['ndvi'] = 15\n",
        "    \n",
        "    total_score = sum(scores.values())\n",
        "    return total_score, scores\n",
        "\n",
        "quality_score, score_breakdown = calculate_quality_score(df)\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('OVERALL DATA QUALITY SCORE')\n",
        "print('='*60)\n",
        "print(f'\\nTotal Score: {quality_score:.1f}/100')\n",
        "print(f'\\nBreakdown:')\n",
        "for component, score in score_breakdown.items():\n",
        "    print(f'  {component.capitalize():15s}: {score:5.1f}')\n",
        "\n",
        "# Quality rating\n",
        "if quality_score >= 90:\n",
        "    rating = 'EXCELLENT'\n",
        "elif quality_score >= 80:\n",
        "    rating = 'GOOD'\n",
        "elif quality_score >= 70:\n",
        "    rating = 'ACCEPTABLE'\n",
        "elif quality_score >= 60:\n",
        "    rating = 'NEEDS IMPROVEMENT'\n",
        "else:\n",
        "    rating = 'POOR'\n",
        "\n",
        "print(f'\\nRating: {rating}')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile all issues\n",
        "issues = []\n",
        "\n",
        "# Missing data issues\n",
        "for _, row in problematic.iterrows():\n",
        "    issues.append({\n",
        "        'severity': 'WARNING',\n",
        "        'category': 'Missing Data',\n",
        "        'description': f\"{row['column']} has {row['missing_pct']:.1f}% missing data\"\n",
        "    })\n",
        "\n",
        "# NDVI issues\n",
        "if ndvi_col:\n",
        "    if ndvi_success_rate < 80:\n",
        "        issues.append({\n",
        "            'severity': 'WARNING',\n",
        "            'category': 'NDVI Quality',\n",
        "            'description': f'NDVI success rate ({ndvi_success_rate:.1f}%) below 80% threshold'\n",
        "        })\n",
        "    if invalid_ndvi > 0:\n",
        "        issues.append({\n",
        "            'severity': 'CRITICAL',\n",
        "            'category': 'NDVI Quality',\n",
        "            'description': f'{invalid_ndvi} NDVI values outside valid range [-1, 1]'\n",
        "        })\n",
        "\n",
        "# Geographic issues\n",
        "if lat_col and lon_col:\n",
        "    if invalid_lat > 0 or invalid_lon > 0:\n",
        "        issues.append({\n",
        "            'severity': 'CRITICAL',\n",
        "            'category': 'Geographic',\n",
        "            'description': f'{invalid_lat + invalid_lon} coordinates outside Wyoming bounds'\n",
        "        })\n",
        "\n",
        "# Print issues by severity\n",
        "issues_df = pd.DataFrame(issues)\n",
        "\n",
        "if len(issues_df) > 0:\n",
        "    print('\\n' + '='*60)\n",
        "    print('DATA QUALITY ISSUES')\n",
        "    print('='*60)\n",
        "    \n",
        "    for severity in ['CRITICAL', 'WARNING', 'INFO']:\n",
        "        severity_issues = issues_df[issues_df['severity'] == severity]\n",
        "        if len(severity_issues) > 0:\n",
        "            print(f'\\n{severity} ({len(severity_issues)}):')\n",
        "            for _, issue in severity_issues.iterrows():\n",
        "                print(f\"  [{issue['category']}] {issue['description']}\")\n",
        "else:\n",
        "    print('\\n✓ No data quality issues detected')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive quality report\n",
        "report = f'''# PathWild Data Quality Report\n",
        "\n",
        "Generated: {pd.Timestamp.now()}\n",
        "\n",
        "## Overall Assessment\n",
        "\n",
        "**Quality Score**: {quality_score:.1f}/100 ({rating})\n",
        "\n",
        "**Score Breakdown**:\n",
        "- Completeness: {score_breakdown['completeness']:.1f}/40\n",
        "- Validity: {score_breakdown['validity']:.1f}/30\n",
        "- Geographic: {score_breakdown['geographic']:.1f}/15\n",
        "- NDVI Quality: {score_breakdown['ndvi']:.1f}/15\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "- Total observations: {len(df):,}\n",
        "- Total features: {df.shape[1]}\n",
        "- Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n",
        "'''\n",
        "\n",
        "if year_col:\n",
        "    year_min = int(df[year_col].dropna().min())\n",
        "    year_max = int(df[year_col].dropna().max())\n",
        "    unique_years_count = df[year_col].dropna().nunique()\n",
        "    report += f'''\n",
        "- Year range: {year_min} to {year_max}\n",
        "- Years covered: {unique_years_count}\n",
        "'''\n",
        "\n",
        "if lat_col and lon_col:\n",
        "    report += f'''\n",
        "- Geographic extent: {df[lat_col].min():.4f}° to {df[lat_col].max():.4f}°N, {df[lon_col].min():.4f}° to {df[lon_col].max():.4f}°W\n",
        "'''\n",
        "\n",
        "report += '''\n",
        "## Issues Summary\n",
        "'''\n",
        "\n",
        "if len(issues_df) > 0:\n",
        "    for severity in ['CRITICAL', 'WARNING', 'INFO']:\n",
        "        severity_issues = issues_df[issues_df['severity'] == severity]\n",
        "        if len(severity_issues) > 0:\n",
        "            report += f'\\n### {severity} ({len(severity_issues)})\\n\\n'\n",
        "            for _, issue in severity_issues.iterrows():\n",
        "                report += f\"- [{issue['category']}] {issue['description']}\\n\"\n",
        "else:\n",
        "    report += '\\nNo issues detected.\\n'\n",
        "\n",
        "report += '''\n",
        "## Recommendations\n",
        "'''\n",
        "\n",
        "if quality_score < 80:\n",
        "    report += '''1. Address critical and warning issues before modeling\n",
        "2. Review outlier records for data collection errors\n",
        "3. Consider imputation strategies for missing data\n",
        "'''\n",
        "else:\n",
        "    report += '''1. Data quality is acceptable for modeling\n",
        "2. Review flagged issues but proceed with caution\n",
        "3. Monitor data quality in production pipeline\n",
        "'''\n",
        "\n",
        "with open(reports_dir / 'quality_report.md', 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f'\\n✓ Saved comprehensive quality report to {reports_dir / \"quality_report.md\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has assessed the quality of the PathWild dataset across multiple dimensions:\n",
        "\n",
        "1. **Missing Data**: Identified columns with high missingness\n",
        "2. **NDVI Quality**: Validated NDVI values and retrieval success\n",
        "3. **Outliers**: Detected extreme values for review\n",
        "4. **Geographic Validation**: Confirmed coordinates within bounds\n",
        "5. **Temporal Coverage**: Assessed consistency over time\n",
        "6. **Feature Ranges**: Validated physically plausible values\n",
        "\n",
        "**Next Steps**:\n",
        "- Review comprehensive quality report in the `data/reports/` directory\n",
        "- Address any CRITICAL issues before proceeding\n",
        "- Proceed to Notebook 07 for feature distribution analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pathwild",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
